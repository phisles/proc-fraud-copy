{
    "filename": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe.pdf",
    "text_by_page": {
        "2": "Small Business Innovation Research(SBIR) Program - Proposal Cover Sheet Disclaimer Knowingly and willfully making any false, fictitious, or fraudulent statements or representations may be a felony under the Federal Criminal False Statement Act (18 USC Sec 1001), punishable by a fine of up to $10,000, up to five years in prison, or both.\nSBIR Phase I Proposal Proposal Number: F244-0001-0110 Proposal Title: Interactive Framework for Real-Time User-Enhanced Knowledge Graphs Agency Information Agency Name: Command: USAF AFMC Topic Number: AF244-0001 Firm Information Firm Name: JDM Solutions, LLC Address: Website: UEI: DUNS: CAGE: 3116 Sandstone Street SE, OWENS CROSS ROADS, AL 35763-7022 www.jdmsolutions.com KQ2AM4W1RTB3 081318619 85B59 SBA SBC Identification Number: 001656321 Firm Certificate OFFEROR CERTIFIES THAT: 1.\nIt has no more than 500 employees, including the employees of its affiliates.\n2.\nNumber of employees including all affiliates (average for preceding 12 months) 3.\nThe business concern meets the ownership and control requirements set forth in 13 C.F.R.\nSection YES 27 YES 121.702.\n4.\nVerify that your firm has registered in the SBAS Company Registry at www.sbir.gov by providing the SBC_001656321 SBC Control ID# and uploading the registration confirmation PDF: Supporting Documentation: \u2022 SBC_001656321 (1).pdf",
        "3": "5.\nIt has more than 50% owned by a single Venture Capital Owned Company (VCOC), hedge fund, or private equity firm 6.\nIt has more than 50% owned by multiple business concerns that are VOCs, hedge funds, or private NO NO equity firms?\n7.\nThe birth certificates, naturalization papers, or passports show that any individuals it relies upon to YES meet the eligibility requirements are U.S.\ncitizens or permanent resident aliens in the United States.\n8.\nIs 50% or more of your firm owned or managed by a corporate entity?\n9.\nIs your firm affiliated as set forth in 13 CFR Section 121.103?\n10.\nIt has met the performance benchmarks as listed by the SBA on their website as eligible to participate 11.\nFirms PI, CO, or owner, a faculty member or student of an institution of higher education NO NO YES NO 12.\nThe offeror qualifies as a: [ ] Socially and economically disadvantaged SBC [ ] Women-owned SBC [ ] HUBZone-owned SBC [ ] Veteran-owned SBC [ ] Service Disabled Veteran-owned SBC [X] None Listed 13.\nRace of the offeror: [ ] American Indian or Alaska Native [ ] Native Hawaiian or Other Pacific Islander [ ] Asian [X] White [ ] Black or African American [ ] Do not wish to Provide 14.\nEthnicity of the offeror: NON- HISPANIC 15.\nIt is a corporation that has some unpaid Federal tax liability that has been assessed, for which all FALSE judicial and administrative remedies have not been exhausted or have not lapsed, and that is not being paid in a timely manner pursuant to an agreement with the authority responsible for collecting the tax liability: 16.\nFirm been convicted of a fraud-related crime involving SBIR and/or STTR funds or found civilly liable NO for a fraud-related violation involving federal funds: 17.\nFirms Principal Investigator (PI) or Corporate Official (CO), or owner been convicted of a fraud-related NO crime involving SBIR and/or STTR funds or found civilly liable for a fraud-related violation involving federal funds: Signature: Printed Name Signature Title Business Name Date Darren Woodruff Darren Woodruf President and CEO JDM Solutions, LLC 01/17/2020 f",
        "4": "Audit Information Summary: Has your Firm ever had a DCAA review?NO VOL I - Proposal Summary Summary: Proposed Base Duration (in months): 6 Technical Abstract: This SBIR Phase I proposal aims to develop a real-time, user-interactive knowledge graph system to enhance situational awareness and decision-making for the U.S.\nAir Force.\nThe system will allow users to interact dynamically with the knowledge graph, enabling them to query, modify, and update graph content to meet evolving operational requirements.\nThis approach prioritizes user-driven interaction and editing, providing tools for users to correct inaccuracies, add context, and enhance the graph's accuracy and utility.\nThe knowledge graph is designed to incorporate data from structured sources, such as the Modernized Integrated Database (MIDB) and Integrated Broadcast Service (IBS), as well as from unstructured or semi- structured sources, like intelligence reports and technical specifications, to ensure a comprehensive view.\nThe primary innovation lies in the system\u2019s use of AI/ML algorithms to enhance and refine graph data based on user input.\nThis includes suggesting modifications, resolving data inconsistencies, and improving data quality through intelligent analysis.\nWhen a user query exposes an information gap, the system can selectively initiate data ingestion from structured and unstructured sources to address that gap, seamlessly integrating relevant information without disrupting operational use.\nThis capability complements the system's focus on interaction, enabling a more accurate and adaptable graph that reflects real- world needs.\nPhase I efforts will concentrate on establishing the foundational architecture to support these user-driven capabilities.\nKey technical objectives include developing a robust framework for interactive graph updates, validating the system\u2019s adaptability to user input and AI-generated insights, and ensuring compatibility with existing Air Force data sources and tools.\nThe successful completion of Phase I will lay the groundwork for Phase II, where the system will scale to support larger datasets and incorporate additional AI/ML capabilities to further enhance data processing and accuracy.\nAnticipated Benefits/Potential Commercial Applications of the Research or Development: The development of a real-time, user-interactive knowledge graph system provides significant benefits for the U.S.\nAir Force",
        "5": "and the broader DoD by enhancing situational awareness and streamlining decision-making.\nThis system enables users to interact with and refine knowledge graph data, allowing for dynamic querying, editing, and updates in response to operational needs.\nBy prioritizing user interaction and adaptable data refinement, the system ensures the knowledge graph remains accurate and relevant, facilitating faster and more informed mission-critical decisions.\nThis solution aligns with the DoD\u2019s strategic goals of increasing agility and supporting data-driven decision-making in complex environments.\nThrough user-driven interactions, the system reduces dependence on manual data management and enables quick corrections to data inconsistencies, ultimately shortening decision cycles.\nAI/ML features improve data integrity by suggesting refinements to the graph, helping users address gaps, resolve conflicts, and update relationships.\nAnticipated benefits include more accurate threat identification, streamlined workflows, and reduced operational delays.\nAdditionally, this technology supports other divisions within AFRL, as noted in out Letter of Support, aligning with efforts in target behavior modeling and multi-domain sensing autonomy.\nBeyond the Air Force, this technology is applicable across federal agencies that rely on large-scale, interconnected datasets.\nIntelligence agencies could use it to maintain dynamic intelligence knowledge graphs, while homeland security and disaster response agencies might leverage it for tracking evolving threats.\nThe system\u2019s flexibility allows it to be tailored for specific operational needs, providing a versatile tool across multiple federal domains.\nIn the private sector, this interactive knowledge graph system can be applied to industries managing complex data environments.\nPharmaceutical companies could use it for monitoring research data and regulatory compliance, financial services firms for market trend analysis and fraud detection, and research organizations for organizing extensive datasets.\nJDM Solutions plans to adapt this technology for commercial markets, offering a secure, scalable solution for real-time data interaction across sectors.\nOur commercialization strategy emphasizes service-based deployment, customization, and ongoing support.\nThe system\u2019s DevSecOps-based microservice architecture enables seamless integration into Joint Service environments, meeting DoD standards and the security requirements of commercial clients.\nBy providing continuous engineering and support services, we build strong client relationships and ensure the system stays responsive to evolving operational needs.\nAttention: Disclaimer: For any purpose other than to evaluate the proposal, this data except proposal cover sheets shall not be disclosed outside the Government and shall not be duplicated, used or disclosed in whole or in part, provided that if a contract is awarded to this proposer as a result of or in connection with the submission of this data, the Government shall have the right to duplicate, use or disclose the data to the extent provided in the funding agreement.\nThis restriction does not limit the Government's right to use information contained in the data if it is obtained from another source without restriction.\nThis restriction does not apply to routine handling of proposals for administrative purposes by Government support contractors.\nThe data subject to this restriction is contained on the pages of the proposal listed on the line below.\nAddition: Enter the page numbers separated by a space of the pages in the proposal that are considered proprietary: List a maximum of 8 Key Words or phrases, separated by commas, that describe the Project:",
        "6": "Dynamic Knowledge Graphs, Real-Time Data Ingestion, AI/ML Data Processing, Situational Awareness, User Interaction, Automated Data Fusion, Decision Support Systems, Operational Intelligence VOL I - Proposal Certification Summary: 1.\nAt a minimum, two thirds of the work in Phase I will be carried out by your small business as defined by 13 C.F.R YES Section 701-705.\nThe numbers for this certification are derived from the budget template.\nTo update these numbers, review and revise your budget data.\nIf the minimum percentage of work numbers are not met, then a letter of explanation or written approval from the funding officer is required.\nPlease note that some components will not accept any deviation from the Percentage of Work (POW) minimum requirements.\nPlease check your component instructions regarding the POW requirements.\nFirm POW Subcontractor POW 100% 0% 2.\nIs primary employment of the principal investigator with your firm as defined by 13 C.F.R Section 701-705?\nYES 3.\nDuring the performance of the contract, the research/research and development will be performed in the YES United States.\n4.\nDuring the performance of the contract, the research/research and development will be performed at the YES offerors facilities by the offerors employees except as otherwise indicated in the technical proposal.\n5.\nDo you plan to use Federal facilities, laboratories, or equipment?\n6.\nThe offeror understands and shall comply with export control regulations.\n7.\nThere will be ITAR/EAR data in this work and/or deliverables.\nNO YES NO 8.\nHas a proposal for essentially equivalent work been submitted to other US government agencies or DoD NO components?\n9.\nHas a contract been awarded for any of the proposals listed above?\n10.\nFirm will notify the Federal agency immediately if all or a portion of the work authorized and funded under this proposal is subsequently funded by another Federal agency.\nNO YES 11.\nAre you submitting assertions in accordance with DFARS 252.227-7017 Identification and assertions use, NO release, or disclosure restriction?\n12.\nAre you proposing research that utilizes human/animal subjects or a recombinant DNA as described in DoDI NO 3216.01, 32 C.F.R.\nSection 219, and National Institutes of Health Guidelines for Research Involving Recombinant DNA of the solicitation: 13.\nIn accordance with Federal Acquisition Regulation 4.2105, at the time of proposal submission, the required YES certification template, \"Contractor Certification Regarding Provision of Prohibited Video Surveillance and Telecommunications Services and Equipment\" will be completed, signed by an authorized company official, and included in Volume V: Supporting Documents of this proposal.",
        "7": "NOTE: Failure to complete and submit the required certifications as a part of the proposal submission process may be cause for rejection of the proposal submission without evaluation.\n14.\nAre teaming partners or subcontractors proposed?\nNO 15.\nAre you proposing to use foreign nationals as defined in 22 CFR 120.16 for work under the proposed effort?\nNO 16.\nWhat percentage of the principal investigators total time will be on the project?\n17.\nIs the principal investigator socially/economically disadvantaged?\n50% NO 18.\nDoes your firm allow for the release of its contact information to Economic Development Organizations?\nYES VOL I - Contact Information Principal Investigator Name: Mr.\nJustin King Phone: (256) 655-4646 Email: Justin.king@jdmsolutions.com Address: 3116 Sandstone Street SE, Hampton Cove, AL 35763 - 7022 Corporate Official Name: Mr.\nDarren Woodruff Phone: (256) 783-4578 Email: darren.woodruff@jdmsolutions.com Address: 3116 Sandstone Street SE, Hampton Cove, AL 35763 - 7022 Authorized Contract Negotiator Name: Mr.\nDarren Woodruff Phone: (256) 783-4578 Email: darren.woodruff@jdmsolutions.com Address: 3116 Sandstone Street SE, Hampton Cove, AL 35763 - 7022 Form Generated on 11/05/2024 11:02:18 PM",
        "8": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 Interactive Framework for Real\u00adTime User\u00adEnhanced Knowledge Graphs Volume 2: Technical Volume 1) Identification and Significance of the Problem or Opportunity The U.S.\nAir Force, along with many other defense and intelligence organizations, relies on vast and complex data sources to maintain situational awareness, conduct pattern of life analysis, and detect emerging threats in real time.\nKnowledge graphs have proven to be an effective tool supporting these missions by capturing and visualizing relationships between entities such as events, locations, people, and objects.\nHowever, the dynamic nature of the operational environment, combined with the rapid pace of military engagements, presents a critical challenge: the need for knowledge graphs that not only adapt in real-time but also incorporate human expertise to ensure accuracy and trust.\nCurrent methods for interacting with knowledge graphs often rely heavily on manual updates or predefined query mechanisms, which are slow and inefficient in time-constrained environments.\nAI and machine learning techniques are employed to automatically structure and update knowledge graphs, but the resulting data is not always trusted by analysts, who may need to correct or enhance the graph manually.\nThis manual process is not only time-consuming but also lacks the ability to automatically propagate changes across the entire knowledge graph.\nThis gap leads to inefficiencies and missed opportunities in rapidly evolving scenarios, where timely and accurate situational awareness is critical for decision-making.\nThe opportunity presented by this challenge is to develop an advanced, AI/ML-powered interactive knowledge graph system that allows users to interact with, modify, and expand the knowledge graph in real time.\nBy leveraging user input, the system will dynamically suggest additional changes to the graph, improving the overall structure, accuracy, and completeness of the data.\nThis capability will significantly reduce the time required for analysts to correct or enhance AI- generated data and allow for a more efficient workflow in support of operations such as threat detection, targeting, and mission planning.\nIn addition, a key capability of our solution will be to not only look inward towards the graph and its existing state, but to also look outward and enable the system to identify data needs and automatically ingest additional data sources to address information gaps.\nThe significance of this effort lies in its potential to greatly enhance the speed and effectiveness of situational awareness and decision-making processes in time-sensitive environments.\nBy allowing analysts to interact with knowledge graphs in a more adaptive and intuitive way, this solution aligns with the Air Force's modernization priorities around trusted AI and autonomy.\nUltimately, this project will provide a powerful tool that not only enhances operational awareness but also enables faster and more accurate responses to emerging threats, making it a critical asset in both military and dual-use applications.\nJDM Solutions, LLC 1",
        "9": "JDM Solutions, LLC 2) Phase I Technical Objectives 2.1 Overview Topic #: AF244-001 Proposal #: F244-0001-0110 The primary objectives of this effort are to develop a flexible and adaptive knowledge graph system that enhances situational awareness through user-driven interactions, machine-driven automation, and dynamic data aggregation.\nSpecifically, the objectives are to: \u2022 Enable Real-Time Interaction with the Knowledge Graph: Develop mechanisms that allow users to interact with the knowledge graph in real time, including the ability to query, modify, and update the graph based on both structured and unstructured data inputs.\n\u2022 Enhance Data Accuracy and Consistency: Implement algorithms that can automatically resolve conflicting or inconsistent data, leveraging subject matter expert (SME) input when necessary, and ensuring the knowledge graph is both accurate and reliable.\n\u2022 Establish and Maintain Dynamic Relationships: Enable the system to automatically create and update relationships between entities based on new information, user input, or changing operational parameters, ensuring that the graph reflects evolving realities.\n\u2022 Trigger Further Data Retrieval and Expansion: Develop mechanisms that trigger additional data retrieval or re-ingestion when the system identifies gaps, conflicting information, or evolving data relationships, allowing for continuous refinement and expansion of the knowledge graph.\nThese objectives will ensure that the system not only meets current situational awareness needs but also provides a scalable, future-proof solution that can adapt to changing operational environments and user requirements.\n2.2 Relationship with AF244\u00ad001 Solicitation Table 1 provides a comprehensive mapping of all explicit requirements outlined in the AF244-001 solicitation to our technical approach.\nAdditionally, we have identified several implied requirements that are necessary to fully meet the research objectives of this effort.\nThese implied requirements address key aspects such as user interaction, system performance, scalability, and security that are critical to developing a robust, user-friendly, and efficient interactive knowledge graph solution.\nEach requirement is mapped to specific Technical Development Objectives (TDOs) and tasks, ensuring that our solution addresses both the immediate needs and long-term goals of the project.\nRequirement Text Description Approach Table 1: Relationship between solicitation DTRA243-003 and this proposal Interactive Knowledge Graph Updates Enable users to interact with and modify the knowledge graph in real time, including adding, updating, and deleting entities and relationships.\nDevelop an intuitive user interface for knowledge graph visualization and interaction, supporting create, update, and delete actions (TDO #1, Tasks 1.1, 1.2).\nIncorporate version control for all graph elements to allow rollback of changes (TDO #3, Tasks 3.1, 3.3).\nAI/ML-Driven Utilize AI/ML to suggest and Implement a Graph Neural Network (GNN) to 2 JDM Solutions, LLC",
        "10": "JDM Solutions, LLC Predictions and Enhancements predict changes to the graph based on user input and newly ingested data.\nTopic #: AF244-001 Proposal #: F244-0001-0110 perform link prediction, node classification, and regression, enhancing graph completeness and accuracy (TDO #2, Tasks 2.1, 2.2).\nIntegrate automated schema updating based on graph topology (TDO #2, Task 2.3).\nInteroperability with NLP Systems Support interaction with text-based data sources for question-answer functionality.\nImplement a natural language interface using LLMs to allow analysts to query the graph using plain language (TDO #1, Task 1.4).\nReal-Time Situational Awareness Support for Time- Based Data Visualization User-Guided Schema Updates Provide timely and accurate updates to the knowledge graph to reflect real-world changes in time- constrained environments.\nEnsure real-time ingestion and updates to the graph, using event-driven microservices to integrate new data streams and automate link creation (TDO #4, Tasks 4.1, 4.2).\nProvide a time-based view of the graph to track changes over time.\nAllow users to modify the underlying schema of the graph to reflect new relationships or entities.\nImplement a time-slider feature for time-based data visualization, enhancing understanding of historical data (TDO #3, Task 3.3).\nDevelop a schema management interface that provides detailed insights into current graph usage, and allow administrators to make global schema updates (TDO #1, Task 1.3).\nUtilize AI to suggest schema changes based on graph data trends (TDO #2, Task 2.3).\nUser-Friendly Interface for Non- Technical Analysts The interface must be intuitive and user-friendly for non-technical users.\nDesign a user-centric interface with visualization features like drag-and-drop, filtering, and search (TDO #1, Task 1.1).\nData Cleaning and Enhancement Automatically clean and augment data as new information is ingested into the graph.\nData Provenance and Auditability Maintain a clear audit trail of all graph changes for data integrity.\nImplement automated cleaning algorithms to detect and merge duplicate nodes and relationships.\nUse LLM-based methods to identify inconsistencies in graph data (TDO #2, Tasks 2.1, 2.2).\nImplement version control and detailed logging of all graph changes, allowing full data provenance (TDO #3, Tasks 3.1, 3.4).\nScalable and Modular Design Ensure the system can handle large-scale data while maintaining modularity and flexibility for future expansion.\nUse a microservice-based framework with containerized components to ensure scalability and integration with other systems (TDO #4, Tasks 4.1, 4.2).\nSecure, DevSecOps- Enabled Deployment Support deployment in secure environments, adhering to DoD DevSecOps guidelines.\nImplement secure, containerized solutions compatible with on-premises and cloud environments.\nMeet DoD Risk Management Framework (RMF) standards (TDO #4, Task 4.3).\nThe proposed solution will significantly enhance the U.S.\nAir Force\u2019s situational awareness by enabling real-time, user-driven modifications to knowledge graphs, ensuring up-to-date and accurate data.\nWith AI/ML automating graph improvements and suggesting updates, analysts will spend less time on manual corrections and more on high-value tasks like threat detection and targeting.\nThe scalable, secure, and modular design ensures future adaptability, while compliance with DoD DevSecOps standards guarantees secure deployment in sensitive environments.\nThis JDM Solutions, LLC 3",
        "11": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 solution will streamline decision-making and improve operational efficiency in time-critical situations.\n2.3 Technical Objectives Our team has identified a set of primary TDO\u2019s needed to support development of a solution for this effort.\nEach TDO is described below, along with the concept for how the objective will be demonstrated at the end of the Phase I effort, to include the data required to support successful development efforts.\nIn addition, Section 3.4 (Statement of Work) provides a Task List along with a detailed description of each development task required to achieve the development objectives.\nTechnical Development Objective #1: Establish a framework capable of supporting creation, deletion, updating, and merging of knowledge graph data.\nTable 2: Technical Development Objective #1 Description: The framework will provide an intuitive visualization of the graph complete with quality-of- life features such as search, sorting and filtering as well as the ingest and aggregation of additional data on demand.\nThis interface will include all features necessary to give an analyst complete control and ownership of the data.\nAs necessary, role-based access control can be placed upon some of the actions such as deletion.\nValidation/Demonstration Concept: Technical Development Objective #2: Establish a system that automates tasks to improve the state of the graph based upon user changes or automated ingestion of data from another software system.\nTable 3: Technical Development Objective #2 Description: Analysts spend time on improving the quality of the graph after systems ingest data into the graph.\nWe propose various methods of automatically cleaning, augmenting, and improving the data to save analysts time, and based upon analysts\u2019 changes.\nValidation/Demonstration Concept: Validate that the added data is correct, and measure the system\u2019s accuracy with 100% accuracy being the goal.\nTable 4: Technical Development Objective #3 Technical Development Objective #3: Time-based version control on all elements in the graph.\nDescription: Version control of the graph data is essential in the case of analyst and machine learning algorithms making edits to the graph.\nThis gives the analyst complete control over the current state of the graph in order to revert any mistakes made by operators or machines.\nValidation/Demonstration Concept: Demonstrate the ability to make changes and then undo them via the user interface.\nDemonstrate the ability to undo changes made by AI/ML.\nTechnical Development Objective #4: Provide DevSecOps enabled containers compatible with objective systems.\nTable 5: Technical Development Objective #4 Description: Provide a set of modular, interoperable, and reusable services to identify, extract, process, and aggregate data.\nDeliverables are intended to be compatible with any other system built using DoD DevSecOps guidance.\nPotential integrations may include cloud as well as on-premises deployment environments.\nThe system will meet all requirements outlined in the rigorous Risk Management Framework (RMF) (DoDI 8510.01).\n1 # O D T 2 # O D T 3 # O D T 4 # O D T JDM Solutions, LLC 4",
        "12": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 Validation/Demonstration Concept: Demonstrate end-to-end operation of a Continuous Integration / Continuous Delivery (CI/CD) pipeline.\nFinal demonstration will show real-time updates of code and build/deployment of all associated containers.\n3) Phase I Statement of Work 3.1 Overall Technical Approach Our overall technical approach is grounded in our experience as both software developers as well as from the perspective of a Lead System Integrator (LSI).\nWe have managed the integration of complex systems which involve both the development of custom code combined with the integration of multiple systems and organizations across DoD, academia and industry.\nThis experience guides our approach, knowing technical development can\u2019t merely stand on its own, but must be integrated with and into larger systems and programs of record to be of value.\nTable 6 describes how we apply this philosophy to guide development through our 4 main focus areas (Mission Performance, Data Interfaces, Architecture and Lifecycle support): Focus Area Approach Approach Rationale Table 61: JDM Solutions Technical Approach Philosophy Mission Performance Well Defined Use Case Data Interfaces Open Standards, REST API Architecture Modular focus, Microservices Lifecycle DevSecOps 3.2 Architecture Collaboratively work with each government customer/stakeholder to define operationally relevant use cases.\nClearly defined use cases are then used to define overall development objectives, required data sets, priorities and assumptions.\nThe use case methodology provides a means to better set and manage expectations across all users and thus better manage risk.\nAll development work is performed with a priority on interoperability \u2013 both from an ingest standpoint and also from an availability perspective to ensure all data and products are exposed for re-use and information sharing.\nPast performance and experience are used along with proven microservices as the basis for interoperability.\nGoal is to provide vendor agnostic solutions with modularity enabled via microservice based development and containerized deployments.\nThe result allows the government to minimize development of custom code and maximize re-use of capabilities throughout existing programs as well as new R&D efforts.\nAgile development methodologies are applied to all projects with a focus on DevSecOps deployments.\nMaximize use of CI/CD pipelines whenever possible.\nThe proposed system architecture (Figure 1) is designed to support dynamic and adaptive knowledge graphs, emphasizing efficient user interaction and continuous refinement.\nAt its core, the architecture allows users to interact with the knowledge graph by querying, modifying, and updating its structure in a streamlined, intuitive manner.\nThe system supports real-time updates based on user input, enhancing the user\u2019s ability to identify, correct, and establish new JDM Solutions, LLC 5",
        "13": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 relationships between entities.\nThis ensures the knowledge graph remains relevant and accurate as operational needs evolve.\nFigure 12: Interactive Framework for Real-Time User-Enhanced Knowledge Graphs A key feature of the architecture is its ability to resolve conflicts within the graph, including erroneous or incomplete data, through user-driven modifications or subject matter expert (SME) feedback.\nThe system also supports version control, enabling users to track changes and revert to previous graph states as needed.\nAdditionally, the architecture allows users to enhance the knowledge graph by adding new relationships or refining existing ones, ensuring continuous improvement in graph quality.\nWhile the focus of this proposal is on user interaction, the architecture also supports automated data ingestion when necessary.\nIf an information gap is detected\u2014whether by the system or through user queries\u2014the system is capable of initiating machine-driven data ingestion from various structured and unstructured sources.\nThis feature ensures that, when needed, the graph can be enriched with new data to provide more comprehensive and actionable insights.\nThis capability, while not the primary focus of this effort, leverages our extensive experience in data ingestion pipelines and adds significant value by enabling seamless integration of new information when operationally relevant.\nBy combining efficient user interaction with automated refinement processes, the architecture provides a scalable, adaptive solution that evolves as new data becomes available and user requirements change.\nThe following sections will explore how the system enables interactive graph editing, incorporates AI/ML techniques for automation, ensures version control, and maintains performance in mission-critical environments.\nJDM Solutions, LLC 6",
        "14": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 3.3 Approach for User Editing the Graph User input methods into a knowledge graph are as myriad as entry into any conventional database.\nWe propose a simple way to enable comprehensive understanding and editing of underlying data in the knowledge graph that will allow for flexibility as well as scalability.\nThe data in the graph can be visualized to the user as nodes and relationships.\nThe user may select any node to edit any property on that node.\nIn addition, the node can be deleted, or a relationship can be added between it and any other node that the user selects.\nLikewise, the user is able to select a relationship in order to edit properties of the relationship, or delete it.\nThis sort of user interface will allow the user to wield full control of the database.\nSince users will be able to perform update and destroy actions on the graph, these actions will all be audited and version controlled.\nThis method allows users or an administrator to restore specific nodes and relationships or even the entire graph into some previous state.\nDynamic version control is critical to this project and is discussed in Section 3.6.\nBy giving analysts direct access to the knowledge graph, they are able to influence the algorithms that utilize the data in the graph.\nFigure 2 displays one of our existing user interfaces for editing a knowledge graph.\nThis graph was generated from an NTSB report about a fire on a passenger vessel.\nThis interface shows the form that allows a user to edit any node\u2019s label and properties, or delete the node.\nThe user can also manually add relationships between nodes.\nFigure 23: Example Interactive User Interface In addition to create, update, and destroy operations for every node and relationship, our framework will provide a variety of other interactive features.\nThe first is an ability to merge duplicate nodes.\nThis is a simple feature on it\u2019s face, but from our experience will save operator\u2019s considerable time.\nThe alternative to this feature is an analyst manually copying and pasting properties from one node to another.\nOnce the properties have been copied, relationships must be manually added before the old node can be deleted.\nIn a future section, we propose a method by which this process could largely be automated.\nAnother key feature for time-series data is a time-slider.\nThis may not be applicable to all knowledge graphs, but given one with a consistent ontology of nodes and relationships that are JDM Solutions, LLC 7",
        "15": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 active from a given point in time to the end of the interval, a time-slider will be able to show the state of the graph at any given point in time.\nIt can also provide an animated view of the evolution of the graph.\nThis dovetails into the time-based version control proposed in a future section.\nLastly, a map view that the analyst can toggle on or off would be beneficial for understanding any geospatial information present in the knowledge graph.\nThe node could be placed on top of the map at it\u2019s given coordinate location while nodes that are not geospatial in nature could be given the ability to float around as the user pans and zooms on the map.\nTo avoid confusion and clutter, nodes without geospatial properties could instead be hidden on the map view.\nThe following figures illustrate this with screenshots of graph data from a previous project.\nThe graph view (Figure 3) showcases interconnected nodes, revealing relationships between the datasets.\nAdditionally, the geospatial view (Figure 4) illustrates the overlapping areas between AIS nodes and hurricane cones on a map.\nThese views provide valuable insights into the correlation between AIS activity and hurricane events.\nFigure 3: Graph View Figure 4: Geospatial View Although the focus of this effort will be the interface for editing the graph, experience shows that a user\u2019s level of understanding of the graph directly correlates with his or her ability to effectively add or edit data in it.\nQuick and advanced filtering capabilities should be provided to whittle down the data that appears on a user\u2019s screen in order to provide focus and clarity.\nIn the same vein, providing a method of interpolating between graph data and natural language would be a force multiplier for both graph understanding and data entry efforts.\nWe discuss this in greater detail in section 3.5.\nThe process of manual modification of graph data discussed in this section is designed to be as efficient as possible so that an analyst does not need to waste precious time learning and using the tool.\nInstead, the tool will drastically speed up the achievement of the analyst\u2019s goals.\nIn order to make updating the knowledge graph as automated as possible, we will introduce machine learning techniques in the next section.\nJDM Solutions, LLC 8",
        "16": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 3.4 Approach for Automated Updating of the Graph with AI/ML Assuming a knowledge graph that is in some state of known accuracy, users gain no utility in providing additional information or updating existing records in the graph.\nNo database \u2013 especially one as dynamic as a knowledge graph \u2013 stays in stasis for long.\nSoon, new information will be discovered and applied to the graph by some automated system, or by an analyst reading an intelligence report.\nThrough the passage of time, some data will become stale and useless.\nIndeed, during a conflict, new intelligence pours in at an impressive rate, and the operations that need to be performed on a knowledge graph in order for it to constantly evolve can hardly keep up.\nThe need for augmenting an analysts speed and accuracy is clear.\nLink prediction is an area of study that has a high volume of algorithms that are successful in gap analysis on the current state of the graph, and generating highly accurate predictions about which nodes should be related and how.\nGiven a knowledge graph with a sufficient amount of data, when an analyst adds some new nodes to the graph, a link prediction algorithm will be able to suggest or automatically apply generated links between the data to form a more complete picture.\nIf a large knowledge graph has never been assessed by a link prediction algorithm, the algorithm will be able to propose likely genuine links that have not been considered before.\nUpon subsequent additions to the graph, the algorithm will not suggest nearly as many changes, but it will be able to provide some suggestions, especially as the number of changes to the graph increases.\nOur approach will utilize a Graph Neural Network (GNN) that will consider both the topology of the graph and properties of the nodes1.\nA GNN trains on the existing data in the graph in order to understand the current topology and properties of the nodes and relationships.\nPart of the training process includes creating embeddings, which allows it to group similar nodes together based upon the properties associated with each node.\nThese embeddings are a key component that enable link prediction and an algorithm called Node Classification.\nOne of the main efforts of researching this topic will be exploring a way to clean data that has been inserted into the knowledge graph.\nIt is essential for a knowledge graph to maintain a consistent ontology of meaningful labels on the nodes and relationships.\nIf a node has been inserted into the graph without a label, an analyst would typically be responsible for cleaning up this data.\nOne way to automate this would be to use a GNN to perform a node classification algorithm on the node.\nThe GNN is able to predict the label by comparing the properties of the node to other nodes with similar properties in order to pick the best label for the entity.\nNo additional steps are required for setting up this GNN to be able to perform node classification other than the steps previously covered in order to prepare the GNN to perform link prediction.\nThe fact that this model has already been trained on the knowledge graph lends credence to its reliability.\nIn a similar vein to node classification, Node Regression is an algorithm that a GNN is able to execute in order to be able to accurately predict node property values.\nIn a case where similar data is being ingested into the graph by different AI/ML methods, running this algorithm on the graph 1 https://arxiv.org/abs/1802.09691 JDM Solutions, LLC 9",
        "17": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 periodically could greatly improve the quality of the nodes by suggesting different property names and values that should have been applied, but were simply missed.\nGraph Neural Networks are capable of providing link prediction, node classification, and node regression algorithms in order to infer additional edges between nodes, decrease gaps in the data, and reduces the overall time spent correcting automated AI/ML methods that have made updates to the graph.\nThese algorithms can be run periodically so that after a number of changes have been made to the graph by analysts or systems, the GNN can suggest additional changes to the graph based upon those updates.\nIn order to understand that a knowledge graph contains conflicting information, the system must understand the ontology and the rules or logic associated with it in an intuitive way.\nThe problem of graph alignment is a different problem than addressed here; however, it is loosely related.\nPart of solving the graph alignment problem seeks to merge two nodes that represent the same entity.\nIn graph alignment, two completely independent graphs are merged together, and it has been shown that LLMs have been a useful tool in solving this problem2.\nIf a node can be converted into a natural language representation of the entity, an LLM can emulate how a human would read a node\u2019s label and properties in order to judge whether or not one duplicates the other.\nIt will be able to suggest that the two nodes be merged, and it can also suggest what the resultant node should look like.\nThe system must be able to provide graph data in a textual format that the LLM can reason over.\nTo accomplish this, the system will split all of the nodes into pages, generate an embedding for each node in the page, and place the embedding into a vector store.\nOnce completed, all of the nodes will have been placed into this vector store.\nThe system iterates through each node in the graph and queries the vector store to see if any other nodes have similar properties that exceed the threshold of 80%.\nIn this manner, we avoid passing the entire graph context into the LLM batch by batch.\nThe system only asks the LLM to reason over potential duplicate nodes.\nOnce matches are found, the nodes will be converted into the well-known GraphSON (graph JSON) format that many LLMs have been trained on.\nThe LLM will be given the task to determine if the nodes are duplicates of the same entity, and will output a merged JSON representation of the node if so.\nFrom here, these results can be stored in a queue for an analyst to approve, or the system could automatically merge these nodes into the suggested representation of the entity.\nThe topic mentioned that the user should be allowed to prompt the modification of the underlying schema of the knowledge graph.\nBefore triggering an update, the user should verify that all software that interacts with the graph will be able to use the new schema.\nTechnically speaking, modification of the graph\u2019s schema is simple.\nIt requires only that a label that is changed from A to B be changed universally throughout the entire graph such that no node retains the label A.\nIn the same way, property names and relationship types can be modified universally.\nAny of these actions should be approved by an administrator due to the downstream ramifications of the 2 https://aidanhogan.com/docs/art_wikidata_kgs_llms.pdf JDM Solutions, LLC 10",
        "18": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 reshaping of the schema; however, it is very possible that this process be suggested or assisted by a machine learning algorithm.\nTo enable this, we suggest that the system extracts the schema from the database and passes it as context to a Large Language Model (LLM).\nAdditionally, the system should extract and pass a reduction of the topology that illustrates how each type of node relates to each other type of node with the frequency of occurrence.\nThen, the LLM can determine if there are any inconsistencies within the structure of the data and suggest alterations to the schema Of course, the system would provide a comprehensive administrator-level user interface to give an administrator granular control over the knowledge graph ontology.\nThis interface would provide the same detailed context to the user, and would allow the user to make global changes upon the schema.\nThe LLM would simply be able to assist or provide suggestions to the user rather than be allowed to make potentially destructive changes directly to the scheme.\nAn illustration of the LLM approach is provided in the Prompt/Response sequence below.\nPrompt Current Schema NODE Labels: Manufacturer, Car, Automobile, Location, Person NODE Properties: [Truncated for brevity] RELATIONSHIP Types: Owns, Produced, Driven_To RELATIONSHIP Properties: [Truncated for brevity] Topology Reduction (:Manufacturer)\u00ad[Produced]\u00ad>(:Car) occurs 20 times (:Manufacturer)\u00ad[Produced]\u00ad>(:Automobile) occurs 200 times (:Person)\u00ad[Owns]\u00ad>(Car) occurs 10 times (:Person)\u00ad[Owns]\u00ad>(Automobile) occurs 100 times (:Automobile)\u00ad[Driven_To]\u00ad>(Location) occurs 50 times (:Car)\u00ad[Driven_To]\u00ad>(Location) occurs 5 times Response The Car label has the same kinds of relationships as the Automobile label, but Car occurs fewer times.\nTherefore, the Car label is redundant.\nI suggest replacing all occurrences of Car with Automobile.\nJDM Solutions, LLC 11",
        "19": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 3.5 Approach for Graph Understanding with AI/ML In order to power a system that allows users to ask questions of the knowledge graph and receive comprehensive answers, we propose providing a user interface to an LLM that operates on the graph data.\nThis interface will transform the graph data into natural language to help the user intuitively understand the data.\nThe user could ask about different aspects of the graph data, or ask for a detailed report to improve situational awareness.\nIn this section, we envision a user who has access to the graph database via user interfaces such as described in the first section, but bear in mind that a report generation feature can apply equally to a user without direct access to the graph data.\nThe Retrieval Augmented Generation (RAG) approach is a method typically applied to textual content of documents that provides context from said documents to the LLM in order to inform the LLM before it provides a response back to the user.\nIn this scenario however, the user is looking at a visualization of the graph onscreen.\nAfter applying any searches, filters, or even by selecting only a few nodes, the user can narrow down the data on screen and ask the LLM a question about that specific data.\nThe system will transform the selected graph data into a well- known format, such as GraphSON, and provide that as context to the LLM alongside the user\u2019s question.\nThis allows the LLM to reason over the data and answer the user\u2019s question about the data.\nA second version of this RAG approach will prove useful when a user is asking a general question about the overall graph.\nThe system will utilize an agentic framework in which an LLM that specializes in a specific task is responsible for completing that task.\nThe system knows that there are a plurality of LLM agents available and what they specialize in, so an LLM will only be asked to complete a task that it is the best at performing.\nThe first stage provides the user\u2019s question to an LLM that specializes in formulating database queries for the knowledge graph, and asks it to generate a database query to perform against the knowledge graph in order to find relevant data.\nThe LLM has also been provided the ontology of the graph as context in order to make a relevant query.\nThe system executes the query and provides the graph data back to another LLM which specializes in reasoning over graph data.\nThus, the user\u2019s question does not need to provide graph data to the system, rather; the system uses the question to find relevant graph data and uses that data to answer the question.\nThis method works in tandem with the visualization effort in order to achieve the essence of what a knowledge graph is: it aids user understanding of complex and dynamic information.\nAdditionally, these insights flow back into the editing and updating efforts, since an analyst cannot modify a knowledge graph without having a sufficient level of understanding of it.\n3.6 Version Control Since automated processes, analysts, and Machine Learning techniques will all be adding new data and updating old data in the knowledge graph, we suggest implementing a version control strategy.\nTime-based versioning is a technique that enables the system to record each state of the data in the graph during a specified time interval for use in version control purposes.\nIt enables an analyst to JDM Solutions, LLC 12",
        "20": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 undo the last action taken in the user interface, as well as an administrator-level ability to reset all nodes and relationships in the entire graph to some past state.\nAdditionally, it functions as an audit log that shows all actions taken with details such as who performed the action when on which node or relationship.\nProvisions must be made in the graph database schema for version control to be a possibility.\nFirst, each node must have it\u2019s state decoupled from its entity.\nIn other words, the State label will be used only for nodes that contain the current state of an entity in the graph.\nAll of the entities in the graph still refer to one another, but the state is stored separately so that it can be updated and archived out-of-band.\nConsider this example node: In order for us to decouple the state from the entity, we add a Has_State relationship between the entity and it\u2019s state node.\nThe entity still retains it\u2019s primary key Id, but the State node has inherited all of the properties.\nThe Has_State relationship has a from and to property which represents the interval during which that state was active within the graph.\nAny relationship where the to field does not exist represents the current state of the entity.\nFigure 4: Sample Node Figure 5: Has_State Relationship When an entity has been updated by an analyst, the current Has_State relationship will have the to property added with the current time-stamp.\nThe new state will be added as a new node in the graph, and a new Has_State relationship between the entity and its state is created as in the previous example.\nJDM Solutions, LLC 13",
        "21": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 Figure 6: Multiple Has_State Relationships An example of two entities relating to each other in the knowledge graph is displayed in Figure 7.\nFigure 8 shows the version controlled approach to this relationship.\nFigure 7: Ownership Relationship Figure 8: Version Controlled Approach Versioning relationships function in the same way that versioning node state does, since the relationship can be considered part of the state.\nEach relationship gets a from date property, and if the relationship expires, it is given a to property.\nTherefore, all current relationships will not have a to property, which is very simple to query for.\nIntuition may lead one to think that these queries will take longer to run, because in order to get the current state of any entity, one extra hop through the most recent Has_State relationship is required.\nThe mitigation to this potential issue is simple.\nSince traversing a relationship in a graph database is incredibly fast \u2013 the operation is O(1) \u2013 the only factor that could slow this query down is having many Has_State relationships due to many edits having taken place on the same node.\nOur team has plenty of experience with the Neo4j graph database, which can index properties.\nIn JDM Solutions, LLC 14",
        "22": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 this case, the to property should be indexed in order to quickly find the most recent State, mitigating any performance concerns.\nWhen using other graph databases, there will be similar techniques to reduce the impact of version control on performance.\nThere are additional ways to potentially improve this design based upon the use-case and how the software uses the graph.\nWe will adjust this model as needed during the phase 1 research effort.\nIt must be emphasized that the previous technical explanation displayed the inner workings of the knowledge graph.\nThe end-user should not be presented with a view of the graph that exposes the version control.\nInstead, it will behave in a user-friendly way in which the user only sees the entities, their properties, and their relationships to one another.\nThis implementation of version control in the graph has been implemented by our developers on an on-going project to enable target system analysis.\nAn underlying knowledge graph contains weapon systems and their components from different military data sources.\nAnalysts that use the software have their changes to the knowledge graph recorded in this manner; thus, a full update history is present on each entity in the graph.\n3.7 Example Workflows In our experience, having well-defined use cases with associated workflows and data flows is critical to the success of any prototyping and R&D effort.\nThese use cases provide practical scenarios that guide the development process and ensure alignment with operational needs.\nBy mapping out how data moves through the system and defining key workflows, we can better address technical challenges and refine the system for scalability and adaptability.\nFor this proposal, we will present two notional use cases that demonstrates the system\u2019s capabilities in handling dynamic knowledge graph interactions, user-driven modifications and identification of data gaps.\nThese sample use cases provide insight into how the system operates under real-world conditions.\nAs part of our Phase I research, we will develop and define multiple additional use cases, expanding on this framework to ensure the system meets a variety of operational scenarios during Phase II Prototyping and Development efforts.\n3.7.1 Use Case #1: Dynamic Knowledge Updates The following workflows highlight the various proposed methods that will be supported for user interaction with the knowledge graph.\nWe feel that this combination of workflows addresses the primary goals for this effort and will provide a baseline for focusing research and development efforts as well as test and validation.\nJDM Solutions, LLC 15",
        "23": "JDM Solutions, LLC #1: Data Visualization Topic #: AF244-001 Proposal #: F244-0001-0110 The figure below shows the workflow for a user to visualize data in the knowledge graph.\nThe user can either pull more data into the user interface, or filter out irrelevant data.\nFigure 9: Data Visualization Workflow JDM Solutions, LLC 16",
        "24": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 #2: System Generated Natural Language Descriptions of the Graph To improve the user\u2019s understanding of the graph, the system will accept questions from the user and process the graph data in order to provide a reasonable breakdown of the data.\nThe user can either request explanations about the graph as a whole, or about specific features of the graph.\nFigure 10: Natural Language Interaction Workflow JDM Solutions, LLC 17",
        "25": "JDM Solutions, LLC #3: Data Editing Topic #: AF244-001 Proposal #: F244-0001-0110 Figure 11 illustrates the workflow of a user creating, editing, deleting, or merging data in the graph.\nAll of these operations finally result in version control being applied to the data no matter what action has been taken so that the previous state can always be recovered.\nFigure 11: User Editing Workflow JDM Solutions, LLC 18",
        "26": "JDM Solutions, LLC #4: Version Control Topic #: AF244-001 Proposal #: F244-0001-0110 The flowchart in figure 12 details the steps that the system takes in order to version control a create, delete, or update action.\nThe underlying model of entity, state, and timestamps ensure that an entity can always be reverted to a previous state.\nFigure 12: Version Control Approach JDM Solutions, LLC 19",
        "27": "JDM Solutions, LLC #5: AI/ML Updating Topic #: AF244-001 Proposal #: F244-0001-0110 Figure 13 displays our AI/ML approach to automatically updating data in the graph.\nThe Graph Neural Network (GNN) is always training on the knowledge graph as more data is added and updated.\nIt performs Node Classification, Node Regression, and Link Prediction algorithms in order to improve the state of the graph without user interaction.\nAn LLM is placed in the loop in order to deduplicate nodes in the graph after updates have taken place.\nFigure 13: Approach for Automated Updates JDM Solutions, LLC 20",
        "28": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 #6: Schema Updates (User and Machine Guided) Both manual and machine-guided approaches to schema updates are illustrated here.\nThe user is given an interface to update the schema and can directly interact with it.\nAlternatively, the user can ask the system to suggest schema updates based upon the current schema and data in the graph.\nIt will either automatically apply these suggestions, or place them into a queue from which the user simply accepts or denies the suggestions.\nFigure 14: Approach for Schema Updates JDM Solutions, LLC 21",
        "29": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 3.7.2 Use Case #2: Identification of Knowledge Gaps Use Case #2 and the associated workflows focus on the identification of data gaps within the Knowledge Graph.\nFrom our experience, we have seen that one of the most limiting factors for an analysis team is ready access to the data needed and the time it takes to identify the proper sources of data, access and the ability to obtain the data.\nWe feel that the ability to aid with rapidly identifying those gaps and helping to establish automated methods to obtain that data is a key operational enabler of this effort.\nFigure 10 and the steps that follow provide a workflow that this effort will enable to address these issues: Figure 15: Sample Workflow for Dynamic Knowledge Graph Updating Step 1: The knowledge graph is initially populated by ingesting data from multiple disparate sources to build the base data model.\nThis includes structured and real-time/streaming data such as the Modernized Integrated Database (MIDB) and Integrated Broadcast Service (IBS), along with semi-structured and unstructured sources like intelligence reports, engineering specifications, and analyst assessments.\nStep 2: Semi-structured and unstructured data is processed to extract relevant entities, activities, and relationships, with initial associations established between them.\nStep 3: The extracted data is used to create or update the ontology and associated schema, which are then input into the knowledge graph.\nStep 4: Streaming and structured data are ingested and fused before being added to the knowledge graph.\n22 JDM Solutions, LLC",
        "30": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 Step 5: The user interacts with the knowledge graph through a graphical user interface (GUI) that presents data as links and nodes, a map for situational awareness, and a conversational interface powered by a large language model (LLM).\nThe user is initially interested in identifying vulnerabilities in a specific type of Integrated Air Defense System (IADS).\nStep 6: After finding relevant information, the user seeks to determine whether any of those systems are operating in a specific area of operations, including any associated activity or patterns of life.\nThe user asks the knowledge graph this question via the conversational interface.\nStep 7: The system identifies the necessary data sources (e.g., MIDB, intelligence reports, and IBS messaging for the given area) and ingests them.\nThe framework processes this data and incorporates it into the knowledge graph by repeating steps 1 through 4.\nStep 8: The newly ingested and processed data is presented to the user in real-time, offering updated insights through the interactive interface.\n3.8 Statement of Work (Task Outline) The following sections detail the technical approach and tasks that will be performed to achieve each of the identified technical development objectives and associated requirements of this effort.\nThis work will also address many Phase II objectives and serve as risk reduction for continued R&D activities.\nTable 7: Development Tasks supporting TDO #1 Technical Development Objective #1 Technical Development Objective #1: Establish a framework capable of supporting creation, deletion, updating, and merging of knowledge graph data.\nDescription: The user interface will provide an intuitive visualization of the graph complete with quality-of-life features such as search, sorting, and filtering.\nThis interface will include all features necessary to give an analyst complete control and ownership of the data.\nAs necessary, role-based access control can be placed upon some of the actions such as deletion.\nTask Description 1.1 Graph Visualization: Visualize the graph in an intuitive manner that allows analysts to explore the data by searching, sorting, and filtering the nodes and relationships.\n1.2 Graph Editing: Enable Create, Update, Merge, and Delete.\nGive analysts the ability to edit every aspect of the data in the knowledge graph.\n1.3 Automated Data Updates: Provide means to add data to the Knowledge Graph on demand via machine- to-machine automation to support user request.\n1.4 UI for Schema Updates: Give administrators the ability to fully update every aspect of the schema.\nThis user interface should show statistics about how the current schema is being utilized, and allows the user to universally apply updates to the schema.\nThe associated AI/ML portion of this effort is addressed in TDO #2.\n1.5 LLM Interaction: Develop an interface by which the analyst can optionally highlight areas of the graph and ask questions about it to the system.\nThe system should respond with reasonable responses about the data in question.\nTable 8: Development Tasks supporting TDO #2 JDM Solutions, LLC 23",
        "31": "JDM Solutions, LLC Technical Development Objective #2 Topic #: AF244-001 Proposal #: F244-0001-0110 Technical Development Objective #2: Establish a system that automates tasks to improve the state of the graph based upon user changes or automated ingestion of data from another software system.\nDescription: Analysts spend time on improving the quality of the graph after systems ingest data into the graph.\nWe propose various methods of automatically cleaning, augmenting, and improving the data to save analysts time, and based upon analysts\u2019 changes.\nTask 2.1 2.2 2.3 Description Automate Gap Analysis: Train a Graph Neural Network (GNN) model on the knowledge graph in order for it to automatically improve the state of the graph by performing link prediction, node classification, and node regression algorithms.\nFind and Merge Duplicate Data: Create a method of utilizing a vector store and LLM to find duplicate data that should be merged in the graph.\nOptionally automate the merging of the data.\nDevelop Method of Updating Schema: Develop a system that suggests schema updates to the user based upon the topology and ontology of the knowledge graph.\nTable 9: Development Tasks supporting TDO #3 Technical Development Objective #3 Technical Development Objective #3: Time-based version control on all elements in the graph.\nDescription: Version control of the graph data is essential in the case of analyst and machine learning algorithms making edits to the graph.\nThis gives the analyst complete control over the current state of the graph in order to revert any mistakes made by operators or machines.\nTask Description 3.1 3.2 3.3 3.4 Define Data Model: Develop a data model that enables time-based version control that can be applied to any knowledge graph.\nApply Data Model to Existing Graph: Demonstrate that the software can take an existing knowledge graph and apply the data model developed in the previous task with the result that version control is applied to the graph.\nUser Interface Integration: Adjust the user interface such that any changes to the graph are version controlled.\nGive analysts the ability to undo changes to the graph.\nAI/ML Integration: Adjust the work outlined in TDO #2 such that any changes to the graph initiated by any AI/ML algorithms are version controlled.\nTable 10: Development Tasks supporting TDO #4 Technical Development Objective #4 Technical Development Objective #4: Provide DevSecOps enabled containers compatible with objective systems.\nDescription: Provide a set of modular, interoperable, and reusable services to identify, extract, process, and aggregate data.\nDeliverables are intended to be compatible with any other system built using DoD DevSecOps guidance.\nPotential integrations may include cloud as well as on-premises deployment environments.\nThe system will meet all requirements outlined in the rigorous Risk Management Framework (RMF) (DoDI 8510.01).\nTask Description JDM Solutions, LLC 24",
        "32": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 4.1 Prototype Delivery Pipeline: Under this task, we will leverage the framework built with Task 1 to establish a Continuous Integration/Continuous Delivery (CI/CD) pipeline to deliver continuous updates throughout the duration of this effort.\nWe will establish a baseline unclassified capability within 6 weeks of contract award that will serve to reduce risk by providing a collaborative environment for the project sponsor, developers, stakeholders and users to assess and interact.\nThis will be the basis for evaluation and feedback throughout the performance of this effort.\n4.3 Baseline Container Delivery: Deliver complete set of baseline containers with all code from Phase I work integrated.\n3.9 Risk Identification and Mitigation Support of any SBIR or R&D related effort has a great degree of uncertainty involved and therefore a large potential degree of risk.\nOur team has demonstrated success leading rapid prototyping efforts for over 25 years.\nIn this time we have identified the primary risk areas that contribute to project failure, along with the mitigation strategies that have been proven to ensure success, particularly for software development projects involving AI/ML algorithm development.\nTable 10 lists the risk areas and associated mitigation plans we feel are of highest potential for this effort.\nRisk and Impact Defined Use Case (Technical, Schedule, Cost) Data Readiness and Availability (Technical) Infrastructure Availability (Schedule and Cost) Analytic Development (Technical) Table 10: Risk Mitigation Approach Mitigation Strategies The single greatest issue we have seen with teams delivering quality products that meet customer needs has been the lack of an established use case that sets expectations between all stakeholders in the project.\nWe will use our proven use case methodology to work with the customer team to define a set of clearly defined end-state objectives and goals that will be used to clearly define all business processes.\n(Refer to Section 3.7 \u2018Example Workflows\u2019 for an overview of proposed Use Cases and data flows).\nAny AI/ML development effort depends on having data available that provides an operationally realistic and relevant challenge.\nOur team has established an extensive collection of open source, notional as well as classified sample data that has been used to support multiple efforts.\nThese sources will greatly reduce the risk of obtaining quality data and the time it takes to locate that data.\nA flexible, scalable and secure environment with automation and collaboration tools is key to any successful development project.\nOur team brings a successful environment that is used to support and manage multiple simultaneous projects.\nAdditionally, our team views core capabilities as an infrastructure commodity that should be leveraged to avoid duplication of effort and wasted cost and schedule.\nMany times the research and design will only provide so many answers and the final design is dictated through prototype development and application of various analytic techniques.\nOur containerized software framework will allow our team to apply a combination of existing, proven algorithms alongside newly developed algorithms and models.\nThis can be done rapidly and iteratively, greatly reducing the risk and allowing our team to apply an agile methodology to the application of potential analytic solutions.\nOverall Project Management (Schedule and Cost) In order to succeed with a rapid prototyping effort, much depends on the proper management structure and team to guide the effort.\nOur team has successfully applied agile development methodologies to multiple projects that have JDM Solutions, LLC 25",
        "33": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 successfully transitioned to operational use and into Programs of Record (PoRs) (reference Section 4 Related Work for specific examples).\nOur team will apply these same proven methods to ensure the success of this effort.\n3.10 Milestone Schedule The total project duration is a period of 6 months from date of award.\nOur proposed schedule to meet all TDO\u2019s is shown in Figure 10.\nThis schedule makes full use of Agile development combined with DevSecOps principles to provide continuous delivery of capabilities.\nThis along with constant interaction with leadership will provide the mechanism to reduce risk and help ensure product expectations are realized.\n3.11 Deliverables Figure 16: Milestone Development Schedule Table 11 identifies a summary of the deliverables proposed for support of this effort.\nAdditional items and modifications will be coordinated with the sponsor after award.\nDeliverable Item Final Report with SF 298 Table 11: Deliverables List Description Final project report documentation detailing the overall project objectives, work performed, results obtained and estimates of technical feasibility.\nThis will also include demonstrations of all capabilities.\nProgress Reports Monthly reports detailing all work performed, plans and potential issues.\nTest Plans Test plans and results for all development.\nJDM Solutions, LLC 26",
        "34": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 Cyber Security Artifacts Baseline code scans for all containers and associated source code.\nCode/Containers All code developed to support Phase II Prototyping.\n4) Related Work Members of the JDM Solutions team have led the design and development of the multiple microservice based frameworks designed to support a variety of mission areas.\nThe organizational objectives have centered on rapidly operationalizing existing Commercial and Open-source technologies to reduce cost and risk.\nTo support this, we have developed services that can be leveraged and combined from various frameworks and used as foundational components to support rapid prototyping activities.\nThis approach has greatly minimized subsequent development timelines and overall cost.\nJDM Solutions have become experts in utilizing graph databases to perform pattern of life analytics, target system analysis, dependency modeling, and correlating military database and streaming track data at scale.\nWe have run graph database clusters in cloud deployments that scale up and out as necessary to handle workload and performance requirements for various customers.\nOur experience as rapid prototyping company has allowed us to experiment with many different graph ontologies and methods of insertion and correlation as well as visualization efforts.\nThese efforts have resulted in multiple successful transitions to operational use.\nFollowing is a description of these efforts: Operational Capability/Transition Description Table 12: JDM Operational Support Air Force Research Lab: Automated/Assisted Target Behavior Model Development (SBIR) Air Force: Intelligent Modeling and Predictive Analysis of C4I Toolkit (IMPACT) Air Force: 612th AOC / DASEE Air Force: Canvas Learning Management System (LMS) Army: Distributed Common Ground Station \u2013 Army (DCGS-A) Summary: JDM led effort to research and develop automated Target System Analysis (TSA) methods utilizing structured, semi-structured, and unstructured data sources processed at speed and scale.\nStatus: Phase I completed; Phase II underway POC: Dr.\nMichael Talbert (michael.talbert@us.af.mil) Summary: JDM led effort to integrate DASEE into the IMPACT program and deploy on Platform One as part of Operational Data Integrated Network (ODIN) Status: Operational POC: Tracy Shelanskey; tracy.shelanskey@caci.com Summary: Information sharing components utilized within 612th AOC to support multi-level information sharing across Unclass, Secret and Secret-REL environments.\nStatus: Operational POC: Curtis Miller; curtis.d.miller34.civ@army.mil Summary: JDM Led effort to migrate Air University to Open-Source version of CANVAS LMS and stand up inside of Cloud One.\nStatus: Operational POC: Matt Fapso; matthew.fapso.1@au.af.edu Summary: Advanced Warfare Environment \u2013 Tactical Geographic Environment (AWarE-TIGER) developed by JDM and transitioned into DCGS-A as the official tool suite to support Air and Missile Defense (AMD) Intelligence analysis Status: Operational POC: Curtis Miller; curtis.d.miller34.civ@army.mil JDM Solutions, LLC 27",
        "35": "JDM Solutions, LLC OSD/Navy Program of Record Army: Air and Missile Defense (AMD) Cloud Environment (ACE) Army: Integrated Air and Missile Defense Planning and Analysis Tool (I- PAT) Marine Corps: Joint Electronic Attack and Compatibility Office (JEACO) / DASEE National Guard Bureau: DOMOPS Awareness and Assessment Response Tool (DAART) Topic #: AF244-001 Proposal #: F244-0001-0110 Summary: DASEE project integrated into OSD-sponsored effort being transitioned to the Navy Status: Planned for operational deployment during FY24 POC: Dave Cox, david.k.cox.civ@army.mil Summary: JDM led effort to develop a threat missile tracking application and automation system for Army Air and Missile Defense Command (AAMDC) Status: Planned for operational deployment during FY24 on Army Cloud at SIPR Level POC: Dave Cox, david.k.cox.civ@army.mil Summary: JDM led effort to provide a air defense asset planning tool suite for the 263rd AAMDC Status: Operational POC: LTC William Westmoreland; william.d.westmoreland10.mil@army.mil Summary: JDM led effort to integrate DASEE services into the Marine Corps Spectral Services Framework (SSF) Status: Operational POC: Dave Cox, david.k.cox.civ@army.mil Summary: JDM led effort to provide collaboration, situational awareness and coordination tools for Defense Support to Civil Authorities (DSCA) missions Status: Operational POC: Curtis Miller; curtis.d.miller34.civ@army.mil AUTOMATED FRAMEWORK FOR TARGET ENRICHMENT AND ACTIVITY MODELING (AFTEAM) Under the SBIR contract for topic AF222-0013, \"Automated/Assisted Target Behavior Model Development,\" our Phase I efforts focused on developing a comprehensive framework and set of services to process structured, semi-structured, and unstructured heterogeneous data at speed and scale.\nThe primary goal was to identify relationships within these data sources to discover and understand patterns of life.\nThe project successfully evaluated and demonstrated the feasibility of aggregating data from diverse sources within a polyglot-based computing environment.\nOur research encompassed both historical and real-time data from disparate sources, including highly structured sensor feeds and geospatial data products, as well as unstructured and semi- structured sources such as news articles and engineering reports.\nThis data was then processed to extract meaningful insights into the behaviors and patterns of targets of interest.\nTo support the enrichment of the AFTEAM knowledge graph, the system utilizes two forms of relationship extraction that operates on any reports imported into the framework.\nThe first approach uses Named Entity Recognition (NER) which is a Natural Language Processing algorithm that finds entities in the text and analysis the grammar to identify dependencies found in the sentences that refer to the entity.\nBy relating the dependencies between entities, the system finds relationships that exist in the textual reporting.\nThese relationships are collated with the relationships found by our second method of extraction: utilizing a Large Language Model (LLM).\nLLMs are good at parsing a report and getting higher level relationships that span larger chunks of text.\nThe LLM outputs the relationships in the GraphSON format, and the system parses, collates, and visualizes the relationships as a graph in the user interface.\nThe user interface allows users to edit the JDM Solutions, LLC 28",
        "36": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 relationships before saving them in the database.\nThis keeps a user in the loop and guards against erroneous data while automating the most tedious task of parsing relationships from a document and entering them into the database.\nThe AFTEAM framework defines a track as a point in time and space at which an entity resided.\nBy looking in documents for information such as latitude, longitude, dates, and times, our NLP method is able to provide the system with track data that can be entered into the AFTEAM database and correlated with entities in the knowledge graph.\nThis component was developed to be generic; therefore, user-defined events can be extracted from the text and provided to any software as JSON data via an event bus.\nBy utilizing a Multi-Modal Large Language Model that can reason about images - also known as a Vision Language Model (VLM) \u2013 the system is no longer limited to only assessing the textual information embedded in Intelligence Community (IC) reporting.\nWhen a report is ingested, the system extracts all images from the report and feeds them to a VLM.\nThe VLM provides a paragraph summary of what is in the image and any text found inside.\nThis information is inserted into the text of the report at the place where the image was extracted from.\nThe effect is that future stages of the NLP pipeline are now able to coherently reason about the images and the text of the report.\nOur current area of research is to provide a graph ontology to an agentic framework.\nWhen the user asks a question about the knowledge graph, the framework uses it\u2019s understanding of the schema to query the database.\nThe results of the query are processed and transformed into natural language that is sent back to the user.\nIn this manner, the user can ask a question about the graph, and the AI agent can look at the graph data and provide an informed answer.\nContract Information \u2022 Prime Contract: FA8650-23-P-1008 (Phase 1); FA2377-24-C-B051 (Phase II) \u2022 Client: AIR FORCE RESEARCH LABORATORY SENSORS DIRECTORATE \u2022 Contact: Michael Talbert, (937) 713-8291; michael.talbert@us.af.mil \u2022 Date of completion: Phase I Completed September 2023, selected for Phase II and received contract award on 9 September, 2024 Design, Development, Demonstration, and Integration (D3I), Missile Defense Prototyping Demonstration (MDPD) Task Order (TO) Members of JDM Solutions serve as the overall Lead Engineer and Lead Software Engineer for the MDPD TO in support of U.S Army Space and Missile Defense Command (USASMDC) Army Capability Manager Space and Missile Defense (ACM SMD).\nIn support of this effort, JDM Solutions is responsible for establishing and maintaining the overall MDPD prototyping environment.\nThis environment consists of a modular software prototyping framework designed to support multiple mission sets, relying heavily on proven Commercial off-the-shelf (COTS) and Government off-the-shelf (GOTS) technologies.\nThis framework was designed and implemented by JDM Solutions and provides a Government owned, non-proprietary container hosting and orchestration environment to support automated/M2M ingest as well as integration with analytics JDM Solutions, LLC 29",
        "37": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 and algorithmic solutions that can be readily applied to any effort.\nHundreds of individual services, tools and containers are available to use as foundational components for development efforts.\nMembers of JDM Solutions work with ACM SMD and their customers to design, develop, and establish solutions that integrate these capabilities to the greatest extent possible, thereby reducing cost, schedule and risk.\nAll products are designed and implemented using the principles set forth in the DoD Enterprise DevSecOps Reference Design to ensure compatibility with Programs of Record (PoR) and readily support technology transfer/transition efforts.\nThis approach aligns the products with DoD policy and ensures the flexibility to support both internal ACM SMD efforts as well as external customer resourced requirements.\nTools and applications range from desktop applications operating without connectivity to cloud deployed solutions on NIPR, SIPR and JWICS.\nThese products are delivered via continuous integration (CI) and continuous delivery/deployment (CD) pipelines that guarantee the products are available to users as quickly as possible.\nThese tools and services are currently used by over 25,000 soldiers, airmen, and first responders to support real-world missions.\nAs part of this effort, JDM Solutions led a team that developed the Digital Attack Surface Execution Environment (DASEE).\nOur team established a DevSecOps ecosystem that enables continuous data discovery, data enrichment and data analysis that combine to produce mission specific courses of action (COAs) that are presented in intuitive manners to support the decision- making process.\nOur work established a government owned framework that readily enables the incorporation of artificial intelligence and machine learning (AI/ML) techniques and processes (e.g., Deep Neural Nets and Graph Analytics), to automatically and rapidly analyze all available data sources.\nJDM Solutions also led the development of the Tactical Feed Service (TFS) within DASEE to ingest a variety of streaming sources, to include Integrated Broadcast Service (IBS), Link-16 and Distributed Interactive Simulation (DIS) data.\nIn addition to structured data sources, our team also led the integration of the Tech Trakr capability to provide government furnished NLP enrichment capabilities for semi-structured and unstructured sources.\nIn support of this work, our team extended the NLP capabilities of Tech Trakr to automatically identify entity level relationships and associate all amplifying data within the DASEE graph.\nContract Information \u2022 Prime Contract: W9113M-16-D-0008 (D3I); Subcontract: 0001098 \u2022 Client: SERCO / U.S Army Space and Missile Defense Command (USASMDC) \u2022 Contact: Dorae Combs; Serco PM; 256-541-0319 ; dorae.d.combs.ctr@army.mil \u2022 Date of completion: Ongoing 5) Relationship with Future Research or Research and Development Anticipated Results: The project will deliver a dynamic, real-time knowledge graph framework designed to support enhanced situational awareness through user-driven data interaction and refinement.\nThis system enables users to actively update and enrich the knowledge graph based on input from subject matter experts (SMEs) from intelligence agencies, engineering fields, and 30 JDM Solutions, LLC",
        "38": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 operational analysts across the services.\nBy integrating insights from these experts, the knowledge graph will achieve a higher level of accuracy and context, aiding faster decision-making and producing more reliable insights.\nThis capability provides a comprehensive, continuously refined view of operational data that directly supports mission-critical objectives for the U.S.\nAir Force.\nSignificance of Phase I for Phase II: Phase I will establish the core architecture for the knowledge graph system, demonstrating its capacity to support real-time user interaction and SME-driven updates.\nThis foundation enables Phase II to expand on these capabilities by integrating advanced AI/ML models that complement SME input and refine data within the graph.\nBy validating this user-focused approach, Phase I sets the groundwork for a scalable solution capable of handling larger datasets and more complex operational requirements.\nPhase II will focus on expanding the system\u2019s adaptability, enhancing AI/ML models to process more diverse data types, and further enabling user contributions to optimize the graph for accuracy and relevance.\nAdditionally, Phase I will help identify any technical challenges, ensuring the system is optimized for performance, scalability, and security as it progresses into more demanding Phase II requirements.\nRequired Clearances and Approvals: JDM Solutions possesses a Top-Secret Facility Clearance and multiple engineers with Secret-level security clearances.\nOur team has extensive experience working within secure environments, and we will work to obtain Authority to Operate (ATO) for any software deployed within classified settings.\nAdditionally, as this project will involve handling sensitive data, we will adhere to strict protocols for data protection, ITAR compliance, and regular cybersecurity audits.\nThese measures will be taken early in Phase II to ensure timely and secure system deployment.\n6) Commercialization Strategy Our commercialization strategy centers on leveraging the unique capabilities of our knowledge graph technology to enhance data interaction and refinement within the Department of Defense (DoD), other federal agencies, and private sector markets.\nBy aligning with current trends in government acquisition and focusing on user-driven data updates, we aim to ensure broad adoption and long-term growth.\nGovernment Adoption and Expansion: Our initial focus is to promote the system\u2019s knowledge graph interaction capabilities, specifically its ability to allow users to efficiently update, query, and refine existing knowledge graphs.\nThese features improve situational awareness by enabling users to work with dynamically updated data in real time, without needing to overhaul the graph ingestion pipeline.\nBy offering the solution through cost-plus or hybrid contract models, we provide a flexible, secure, and adaptable system that can fit seamlessly into any Joint Service environment.\nFollowing DevSecOps guidance from the Office of the Secretary of Defense (OSD), the system will be delivered as microservices in secure, containerized deployments.\nThese containers are fully compatible with DoD infrastructure and can run in any cloud or on-premise setup, providing additional value by promoting interoperability across different services.\nLeveraging our ongoing Phase II AFTEAM SBIR work, we will secure early adopters within the Air Force Research Laboratory (AFRL) and other DoD agencies.\nJDM Solutions, LLC 31",
        "39": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 Engineering Services and Support: Our monetization strategy prioritizes engineering services, including deployment, customization, and ongoing support, over traditional software licensing fees.\nBy offering continuous service and integration support, we create sustained revenue streams while ensuring that the technology remains optimized for evolving customer needs.\nThis approach fosters long-term partnerships with government customers and aligns with the DoD\u2019s preference for service-based contracts.\nOur commitment to following DevSecOps principles ensures that the system is secure, maintainable, and rapidly deployable within mission-critical environments.\nPrivate Sector Adaptation: In the private sector, we will adapt the technology to industries requiring secure, real-time data interaction, such as pharmaceuticals, financial institutions, and research organizations.\nThe system\u2019s knowledge graph interaction and refinement capabilities, developed for military use, will provide a scalable solution for managing large, complex datasets.\nWe will focus on delivering containerized microservices that fit within the cybersecurity requirements of each industry, ensuring that our solutions are adaptable to different regulatory environments.\nPartnerships with system integrators and consulting firms will help us reach private sector clients and offer solutions tailored to their specific needs.\nCommercialization Schedule and Goals: \u2022 Year 1-2: Focus on securing initial contracts within AFRL and expanding to other DoD agencies.\nTarget two major contracts while beginning outreach to private sector clients to identify early adopters.\n\u2022 Year 3-4: Expand our footprint within the DoD and federal agencies, with a goal of securing contracts worth $5 million annually.\nLaunch marketing efforts in the private sector, emphasizing industries identified in our market analysis.\n\u2022 Year 5 and Beyond: Establish the solution as a leading platform for knowledge graph interaction and refinement in both public and private sectors.\nAchieve a cumulative $20 million in service-based revenue through engineering services and support by the end of Year 5.\nBy aligning our commercialization strategy with the DoD\u2019s evolving acquisition trends and leveraging opportunities in the private sector, we aim to maximize the impact of our knowledge graph technology.\nOur adherence to DevSecOps standards and microservice-based delivery ensures a secure, scalable, and sustainable platform for both government and commercial markets.\n7) Key Personnel JDM Solutions has assembled a team to support this effort with both extensive technical and operational experience.\nOur view is that technology development is only of real value if it is put in the hands of analysts and warfighters.\nOur team has demonstrated experience making this happen, through development of operationally relevant capabilities created using industry best practices and delivered as modular microservices that can be used in any system or application.\nJDM Solutions, LLC 32",
        "40": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 Justin King (Chief Technology Officer, JDM Solutions) Summary: Mr.\nJustin King has over 19 years of experience in designing and implementing advanced software solutions, with a strong focus on artificial intelligence (AI) and machine learning (ML) technologies, large language models (LLMs), and natural language processing (NLP).\nHis expertise lies in developing scalable, modular solutions that align with DoD DevSecOps guidance, enabling seamless integration and reuse across multiple government projects.\nMr.\nKing\u2019s extensive technical background includes leading initiatives in AI/ML model development, graph computing, cloud-based deployments, and microservice architecture.\nHis recent work emphasizes the application of LLMs for processing structured, semi-structured, and unstructured data to enhance decision-making processes across defense and intelligence domains.\nThroughout his career, Mr.\nKing has consistently delivered high-impact solutions that meet critical customer needs while ensuring the scalability and adaptability of systems for long-term use.\nHe has successfully integrated AI/ML models and NLP techniques to support large-scale data processing, including extracting meaningful insights from complex, multi-modal datasets.\nRelevant Experience: AI/ML and LLM Expertise: Mr.\nKing leads research efforts that apply cutting-edge AI and ML technologies, including the development of LLM-powered pipelines for document processing, entity recognition, and relationship extraction from large scientific and intelligence datasets.\nHis recent work with multi-modal large language models (Vision Language Models) demonstrates his ability to extract and synthesize both text and image data from various sources, enhancing the utility of complex document repositories.\nMr.\nKing\u2019s work integrates text and image processing using AI/ML models, enabling the system to handle various data formats and enhance the operational efficiency of document analysis workflows.\nLead Engineer, Digital Attack Surface Execution Environment (DASEE): Mr.\nKing played a pivotal role in the development and successful transition of the DASEE platform, integrating AI/ML techniques and NLP capabilities for analyzing high-volume, multi-source data.\nHis contributions to this platform include developing scalable microservices for automated data processing, event extraction, and knowledge graph generation.\nNamed Entity Recognition (NER) and NLP Pipelines: Mr.\nKing\u2019s leadership in NLP projects has enabled the automated extraction and analysis of entity relationships from intelligence reports and technical documents.\nBy leveraging NER techniques and LLM-based methods, he has driven advancements in data enrichment, improving the accuracy and speed of decision-making processes in intelligence and national security applications.\nGraph Computing and Knowledge Graphs: A key focus of Mr.\nKing\u2019s work has been the development of graph-based solutions for understanding complex relationships in large datasets.\nHis expertise in knowledge graph generation, supported by AI-driven entity extraction, has provided valuable insights in domains such as threat analysis and pattern recognition.\nEducation: B.S.\nComputer Science, Auburn University Role: Mr.\nKing will serve as the Principal Investigator for this SBIR Phase II effort, responsible 33 JDM Solutions, LLC",
        "41": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 for overall system design and implementation.\nHe will lead the integration of AI/ML models, focusing on NLP and multi-modal document processing, ensuring the system\u2019s scalability and effectiveness in meeting DTRA\u2019s needs.\nDarren Woodruff (President, JDM Solutions) Summary: Mr.\nWoodruff has over 30 years of experience leading the development and fielding of Software Applications and Integrated Systems across the DoD and Joint/Interagency community.\nHe has an accomplished record of developing and delivering systems on time and within budget using agile development principles and has managed the development of multiple Joint C2 and Intelligence systems which are being used extensively in the field today.\nHe has been responsible for managing and coordinating all aspects of development, testing, accreditation, fielding and support of multiple C2 and C4I prototype systems that support the Multi-Domain mission set, to include the DOMOPS Awareness and Assessment Response Tool (DAART) Suite, the Digital Attack Surface Execution Environment (DASEE) as well as the Advanced Warfare Environment (AWarE), Tactical Geographic Environment (TIGER) and the Air and Missile Defense Explorer (AMD-E) Suite.\nHe has led the transition of these efforts Programs of Record (PoRs) to support operational use, with daily use by over 25,000 registered users across multiple services and agencies.\nThese include the development and successful transitions to the Distributed Common Ground Station-Army (DCGS-A), Air Force IMPACT and Marine Core MCSIL/SSF.\nMr.\nWoodruff will use this experience to help manage development efforts for this effort to ensure that the results provide needed capabilities for the Insight Program as well as other programs throughout the DoD, Education: B.S., Electrical Engineering Technology, University of Alabama Role: Mr.\nWoodruff will serve as the overall Program Manager and work with customers and stakeholders as the Transition Lead to ensure that technologies developed as part of this effort are successfully adopted.\n8) Foreign Citizens No foreign citizens or individuals holding dual citizenship will be involved in this effort.\n9) Facilities/Equipment JDM Solutions currently supports multiple software development and prototyping efforts from our location at 100 Quality Circle, Suite 200 Huntsville, AL 35806.\nWe utilize both on-premises hardware and a secure cloud based development environment hosted within AWS GovCloud (US) Region that will be available to support this effort.\nThis GovCloud environment will soon have all the controls needed for Cybersecurity Maturity Model Certification (CMMC) and FAR 252.239-7010 compliance.\nThe facilities meet all environmental laws and regulations for the State of Alabama for, but not limited to: airborne emissions, waterborne effluents, external radiation levels, outdoor noise, solid JDM Solutions, LLC 34",
        "42": "JDM Solutions, LLC Topic #: AF244-001 Proposal #: F244-0001-0110 and bulk waste disposal practices, and handling and storage of toxic and hazardous materials.\n10) Subcontractors/Consultants None 11) Prior, Current, or Pending Support of Similar Proposals or Awards No prior, current, or pending support for proposed work.\n12) Technical Data Rights JMD Solutions does not assert any restrictions on the technical data, including software, developed under this contract.\nWe believe in enabling broad reuse and collaboration across the U.S.\ngovernment to maximize the value of the technology developed under this project.\nAll technical data and software created will be provided to the government without any restrictions, allowing for unrestricted use, release, and disclosure by the government.\nBy opting for unrestricted rights, we aim to align with the Department of Defense\u2019s (DoD) shift toward more flexible acquisition strategies and to encourage widespread adoption of our technology.\nThis approach also allows for the government and its partners to implement, modify, and extend the software as needed to meet evolving mission requirements without any licensing barriers.\nOur focus will be on providing engineering services to support the deployment, customization, and ongoing maintenance of the technology, rather than monetizing the software through licensing fees.\n13) Identification and Assertion of Restrictions on the Government\u2019s Use, Release or Disclosure of Technical Data or Computer Software Technical Data or Computer Software to be Furnished with Restrictions Basis for Assertion Asserted Rights Category Name of Person or Organization Asserting Restrictions None NA None JDM Solutions Note: By entering \"None,\" we assert that there are no restrictions on the use, release, or disclosure of any technical data or software developed under this contract.\nJDM Solutions, LLC 35",
        "43": "SBIR Phase I Proposal Proposal Number Topic Number Proposal Title Date Submitted Firm Information Firm Name Mail Address F244-0001-0110 AF244-0001 Interactive Framework for Real-Time User-Enhanced Knowledge Graphs 11/05/2024 11:02:15 PM JDM Solutions, LLC 3116 Sandstone Street SE, OWENS CROSS ROADS, Alabama, 35763 Website Address www.jdmsolutions.com UEI Duns Cage KQ2AM4W1RTB3 081318619 85B59 Total Dollar Amount for this Proposal Base Year Year 2 Technical and Business Assistance(TABA)- Base TABA- Year 2 Base Year Summary Total Direct Labor (TDL) Total Direct Material Costs (TDM) Total Direct Supplies Costs (TDS) Total Direct Equipment Costs (TDE) Total Direct Travel Costs (TDT) Total Other Direct Costs (TODC) G&A (rate 13.14%) x Base (TDL+TOH) Total Firm Costs Subcontractor Costs Total Subcontractor Costs (TSC) Cost Sharing Profit Rate (10%) Total Estimated Cost TABA Year 2 Summary Total Direct Labor (TDL) Total Direct Material Costs (TDM) $137,849.80 $137,849.80 $0.00 $0.00 $0.00 $110,763.66 $0.00 $0.00 $0.00 $0.00 $0.00 $14,554.34 $125,318.00 $0.00 -$0.00 $12,531.80 $137,849.80 $0.00 $0.00 $0.00",
        "44": "Total Direct Supplies Costs (TDS) Total Direct Equipment Costs (TDE) Total Direct Travel Costs (TDT) Total Other Direct Costs (TODC) G&A (rate 13.14%) x Base (TDL+TOH) Total Firm Costs Subcontractor Costs Total Subcontractor Costs (TSC) Cost Sharing Profit Rate (10%) Total Estimated Cost TABA Base Year Direct Labor Costs Category / Individual-TR Computer and Information Research Scientist/ Principal Investigator (Justin King) Computer Programmer/ UI/UX Lead Software Developer/ LLM Lead Engineers, All Other/ DevSecOps Engineer Subtotal Direct Labor (DL) Labor Overhead (rate 53.71%) x (DL) Total Direct Labor (TDL) G&A (rate 13.14%) x Base (TDL+TOH) Cost Sharing Profit Rate (10%) Total Estimated Cost TABA Year 2 Direct Labor Costs $0.00 $0.00 $0.00 $0.00 $0.00 $0.00 $0.00 -$0.00 $0.00 $0.00 $0.00 Rate/Hour Estimated Hours Fringe Rate (%) Fringe Cost Cost $102.76 355 $62.99 210 $52.98 190 $98.29 125 $36,479.80 $13,227.90 $10,066.20 $12,286.25 $72,060.15 $38,703.51 $110,763.66 $14,554.34 -$0.00 $12,531.80 $137,849.80 $0.00 Category / Individual-TR Rate/Hour Estimated Hours Fringe Rate (%) Fringe Cost Cost Computer and Information Research $102.76 0 $0.00",
        "45": "$62.99 $52.98 $98.29 0 0 0 Scientist/ Principal Investigator (Justin King) Computer Programmer/ UI/UX Lead Software Developer/ LLM Lead Engineers, All Other/ DecSecOps Lead Subtotal Direct Labor (DL) Labor Overhead (rate 53.71%) x (DL) Total Direct Labor (TDL) G&A (rate 13.14%) x Base (TDL+TOH) Cost Sharing Profit Rate (10%) Total Estimated Cost TABA $0.00 $0.00 $0.00 $0.00 $0.00 $0.00 $0.00 -$0.00 $0.00 $0.00 $0.00 Explanatory Material Relating to the Cost Volume The Official From the Firm that is responsible for the cost breakdown Name: Darren Woodruff Phone: (256) 783-4578 Phone: darren.woodruff@jdmsolutions.com Title: Proposal Owner If the Defence Contracting Audit Agency has performed a review of your projects within the past 12 months, please provide: No Select the Type of Payment Desired: Partial payments",
        "46": "Cost Volume Details Direct Labor Base Category Description Education Yrs Experience Hours Rate Fringe Rate Total Computer and Information Research Scientist Principal Investigator Bachelor's Degree Computer Programmer UI/UX Lead Software Developer LLM Lead Bachelor's Degree Bachelor's Degree Engineers, All Other DevSecOps Engineer Bachelor's Degree 20 7 4 30 355 $102.76 $36,479.80 210 $62.99 $13,227.90 190 $52.98 $10,066.20 125 $98.29 $12,286.25 Are the labor rates detailed below fully loaded?\nNO Provide any additional information and cost support data related to the nature of the direct labor detailed above.\nThe labor rates provided above are hourly with all burden being applied through OH and G&A rates specified.\nAll proposed employees are currently approved and working on DoD contracts with these rates Labor rate Documentation: \u2022 Cost Narrative - JDM Solutions F244-0001-0110.pdf Direct Labor Cost ($): Year2 $72,060.15 Category Description Education Yrs Experience Hours Rate Fringe Rate Total Computer and Information Research Scientist Principal Investigator Bachelor's Degree Computer Programmer UI/UX Lead Software Developer LLM Lead Bachelor's Degree Bachelor's Degree 20 7 4 0 0 0 $102.76 $62.99 $52.98 $0.00 $0.00 $0.00",
        "47": "Engineers, All Other DecSecOps Lead Bachelor's Degree 30 0 $98.29 $0.00 Are the labor rates detailed below fully loaded?\nNO Provide any additional information and cost support data related to the nature of the direct labor detailed above.\nThe labor rates provided above are hourly with all burden being applied through OH and G&A rates specified.\nAll proposed employees are currently approved and working on DoD contracts with these rates Direct Labor Cost ($): Sum of all Direct Labor Costs is($): Overhead Base Labor Cost Overhead Rate (%) Overhead Comments: As a small business the company has two final indirect cost pools, Overhead (OH) and General and Administrative (G&A), and two intermediate pools for Fringe Benefits and Facilities that are allocated to the final cost pools.\nBurden rates are applied on a company wide basis and are based upon JDM Solutions\u2019 historical data and the current year budget, management\u2019s best estimate of pool expenses and associated costs for that pool for the current year.\nJDM Solutions does not have a Forward Pricing Rate Agreement with the Government.\nThe burden rates used in this proposal are consistent with rates used in other recent proposals to the Department of Defense customers.\nOur accounting system has not yet been audited by DCAA to determine adequacy.\nAll ICE submissions have been prepared and submitted to the cognizant agency.\nOverhead: The cost elements in the Overhead pool include Fringe Expense allocations and Indirect Expenses associated with the management of contract efforts.\nThe Overhead Pool also includes an allocation for Facilities as well as any cost deemed to be indirect.\nOverhead Cost ($): Year2 Labor Cost Overhead Rate (%) Overhead Comments: As a small business the company has two final indirect cost pools, Overhead (OH) and General and Administrative (G&A), and two intermediate pools for Fringe Benefits and Facilities that are allocated to the final cost pools.\nBurden rates are applied on a company wide basis and are based upon JDM Solutions\u2019 historical data and the current year budget, management\u2019s best estimate of pool expenses and associated costs for that pool for the current year.\nJDM Solutions does not have a Forward Pricing Rate Agreement with the Government.\nThe burden rates used in this proposal are consistent with rates used in other recent proposals to the Department of Defense customers.\nOur accounting system has not yet been audited by DCAA to determine adequacy.\nAll ICE submissions have been prepared and submitted to the cognizant agency.\nOverhead: The cost elements in the Overhead pool include Fringe Expense allocations and Indirect Expenses associated with the management of contract efforts.\nThe Overhead Pool also includes an allocation for Facilities as well as any cost deemed to be indirect.\n$0.00 $72,060.15 53.71 $38,703.51 53.71",
        "48": "Overhead Cost ($): Sum of all Overhead Costs is ($): General and Administration Cost Base G&A Rate (%): Apply G&A Rate to Overhead Costs?\nApply G&A Rate to Direct Labor Costs?\nPlease specify the different cost sources below from which your company's General and Administrative costs are calculated.\nG&A costs are necessary costs that contribute to the overall operations of the company but are not directly attributable to a specific contract or cost objective.\nThese costs are collected in JDM\u2019s G&A pool and are allocated to contracts based on total costs.\nG&A Cost ($): Year2 G&A Rate (%): Apply G&A Rate to Overhead Costs?\nApply G&A Rate to Direct Labor Costs?\nPlease specify the different cost sources below from which your company's General and Administrative costs are calculated.\nG&A costs are necessary costs that contribute to the overall operations of the company but are not directly attributable to a specific contract or cost objective.\nThese costs are collected in JDM\u2019s G&A pool and are allocated to contracts based on total costs.\nG&A Cost ($): Sum of all G&A Costs is ($): Profit Rate/Cost Sharing Base Cost Sharing ($): Cost Sharing Explanation: $0.00 $38,703.51 13.14 YES YES $14,554.34 13.14 YES YES $0.00 $14,554.34 -$0.00",
        "49": "Profit Rate (%): Profit Explanation: Total Profit Cost ($): Year2 Cost Sharing ($): Cost Sharing Explanation: No cost sharing is proposed Profit Rate (%): Profit Explanation: The company has assessed the degree of risk for this contract based on our understanding of the Performance Work Statement.\nFactors that were taken into consideration in the development of this fixed fee are: 1) the complexity of the PWS which requires personnel with exceptional abilities and professional credentials; 2) contains new and emerging technology that results in increased technical performance and systems that contain significant technical advances.\nOur company has proven results in its ability to meet these types of complexity factors with highly skilled employees, as well, as control contract costs.\nGiven the apportionment of risk to our Company and our ability to perform in this environment, we believe the proposed fixed fee amount to be fair and reasonable.\nTotal Profit Cost ($): Total Proposed Amount ($): 10 $12,531.80 - 10 $12,531.80 $137,849.80",
        "50": "Darren Woodruff, JDM Solutions, LLC Nov 05, 2024 Nov 05, 2025"
    },
    "images": [
        {
            "page": 13,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page13_img1.png",
            "hash": "e98dab8974568696",
            "position": "Top Center"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page14_img1.png",
            "hash": "c0a7774343392e9d",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page15_img1.jpeg",
            "hash": "c4c33a63e1a566da",
            "position": "Middle Left"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page15_img2.jpeg",
            "hash": "c363399f6c324b34",
            "position": "Middle Right"
        },
        {
            "page": 20,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page20_img1.png",
            "hash": "cff5912a34d2d22c",
            "position": "Middle Center"
        },
        {
            "page": 20,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page20_img2.png",
            "hash": "ddb3e2c4ca80c3b5",
            "position": "Top Right"
        },
        {
            "page": 21,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page21_img1.png",
            "hash": "eb3f4a6590908dce",
            "position": "Top Center"
        },
        {
            "page": 21,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page21_img2.png",
            "hash": "c857d9e869e089ad",
            "position": "Middle Center"
        },
        {
            "page": 21,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page21_img3.png",
            "hash": "9a6187dea0b5aa5a",
            "position": "Middle Center"
        },
        {
            "page": 23,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page23_img1.png",
            "hash": "f3f7b202734a3494",
            "position": "Middle Center"
        },
        {
            "page": 24,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page24_img1.png",
            "hash": "b49998c3563c3c76",
            "position": "Middle Center"
        },
        {
            "page": 25,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page25_img1.png",
            "hash": "87a5d2b307e6522d",
            "position": "Middle Center"
        },
        {
            "page": 26,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page26_img1.png",
            "hash": "d3adcc23d28c0e5b",
            "position": "Middle Center"
        },
        {
            "page": 27,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page27_img1.png",
            "hash": "f8c2872d525aada5",
            "position": "Middle Center"
        },
        {
            "page": 28,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page28_img1.png",
            "hash": "8db28dcd06f64687",
            "position": "Middle Center"
        },
        {
            "page": 29,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page29_img1.png",
            "hash": "e12dabab745a8e84",
            "position": "Middle Center"
        },
        {
            "page": 33,
            "image_file": "docNAC01FB74535330a17d13c55f0c44c686e2107b0b075e24ed2c6e3d0917e9133ea968fe431afe_page33_img1.png",
            "hash": "b738586e67433171",
            "position": "Middle Center"
        }
    ],
    "firm_info": {
        "company": "N/A",
        "address": "N/A",
        "website": "jdmsolutions.com",
        "name": "N/A",
        "phone": "N/A"
    }
}