{
    "filename": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385.pdf",
    "text_by_page": {
        "2": "Small Business Innovation Research(SBIR) Program - Proposal Cover Sheet Disclaimer Knowingly and willfully making any false, fictitious, or fraudulent statements or representations may be a felony under the Federal Criminal False Statement Act (18 USC Sec 1001), punishable by a fine of up to $10,000, up to five years in prison, or both.\nSBIR Phase I Proposal Proposal Number: F244-0001-0057 Proposal Title: AI-Generated Dynamic Knowledge Graph for Situational Awareness Agency Information Agency Name: Command: USAF AFMC Topic Number: AF244-0001 Firm Information Firm Name: AI Sensation Address: Website: UEI: CAGE: 26766 Ashford, MISSION VIEJO, CA 92692-0000 http://www.ai-sensation.com/ W2XXHSDNL8A4 9RE77 SBA SBC Identification Number: 002650345 Firm Certificate OFFEROR CERTIFIES THAT: 1.\nIt has no more than 500 employees, including the employees of its affiliates.\n2.\nNumber of employees including all affiliates (average for preceding 12 months) 3.\nThe business concern meets the ownership and control requirements set forth in 13 C.F.R.\nSection YES 6 YES 121.702.\n4.\nVerify that your firm has registered in the SBAS Company Registry at www.sbir.gov by providing the SBC_002650345 SBC Control ID# and uploading the registration confirmation PDF: Supporting Documentation: \u2022 Proof of SCB.pdf 5.\nIt has more than 50% owned by a single Venture Capital Owned Company (VCOC), hedge fund, or NO",
        "3": "private equity firm 6.\nIt has more than 50% owned by multiple business concerns that are VOCs, hedge funds, or private NO equity firms?\n7.\nThe birth certificates, naturalization papers, or passports show that any individuals it relies upon to YES meet the eligibility requirements are U.S.\ncitizens or permanent resident aliens in the United States.\n8.\nIs 50% or more of your firm owned or managed by a corporate entity?\n9.\nIs your firm affiliated as set forth in 13 CFR Section 121.103?\n10.\nIt has met the performance benchmarks as listed by the SBA on their website as eligible to participate 11.\nFirms PI, CO, or owner, a faculty member or student of an institution of higher education NO NO YES YES 12.\nThe offeror qualifies as a: [X] Socially and economically disadvantaged SBC [ ] Women-owned SBC [ ] HUBZone-owned SBC [ ] Veteran-owned SBC [ ] Service Disabled Veteran-owned SBC [ ] None Listed 13.\nRace of the offeror: [X] American Indian or Alaska Native [ ] Native Hawaiian or Other Pacific Islander [ ] Asian [ ] White [ ] Black or African American [ ] Do not wish to Provide 14.\nEthnicity of the offeror: NON- HISPANIC 15.\nIt is a corporation that has some unpaid Federal tax liability that has been assessed, for which all FALSE judicial and administrative remedies have not been exhausted or have not lapsed, and that is not being paid in a timely manner pursuant to an agreement with the authority responsible for collecting the tax liability: 16.\nFirm been convicted of a fraud-related crime involving SBIR and/or STTR funds or found civilly liable NO for a fraud-related violation involving federal funds: 17.\nFirms Principal Investigator (PI) or Corporate Official (CO), or owner been convicted of a fraud-related NO crime involving SBIR and/or STTR funds or found civilly liable for a fraud-related violation involving federal funds: Signature: Printed Name Signature Title Business Name Date Mohsen Imani Mohsen Imani President AI Sensation 06/09/2024",
        "4": "Audit Information Summary: Has your Firm ever had a DCAA review?NO VOL I - Proposal Summary Summary: Proposed Base Duration (in months): 6 Technical Abstract: This project proposes the development of an AI-Generated Dynamic Knowledge Graph to enhance real-time situational awareness in complex naval and defense scenarios.\nThe system will leverage cutting-edge Large Language Models (LLMs) and Graph Neural Networks (GNNs) to dynamically integrate and analyze multimodal sensor inputs, including camera, radar, and depth sensors.\nThe goal is to enable faster, more accurate threat detection, and optimize decision-making in time-sensitive environments.\nThe core innovation lies in the system's semantic-level fusion of data from diverse sensor modalities, allowing operators to interact with a transparent and interpretable knowledge graph that continuously updates based on real-time information.\nThe framework ensures scalable, mission-critical performance by dynamically generating relationships between entities such as vessels, aircraft, and potential threats, while accounting for sensor resolution, proximity, and mission context.\nFurthermore, it supports intelligent tracking of threat indicators with minimal operator input, enhancing situational awareness without overwhelming the user with excessive data.\nThis Phase I effort will focus on building the system\u2019s core components, including a robust pipeline for real-time sensor fusion and the development of an intuitive user interface that allows operators to interact with the knowledge graph and modify relationships as new data emerges.\nThe project will deliver a working prototype and demonstrate the feasibility of this approach, highlighting its potential for seamless integration into existing defense systems.\nThe system\u2019s impact is expected to extend beyond defense applications, offering broad utility in any environment requiring dynamic, data-driven situational awareness, from disaster response to critical infrastructure monitoring.\nPhase I will validate the system\u2019s performance, establishing key benchmarks for threat detection accuracy, graph update latency, and user interaction speed.\nAnticipated Benefits/Potential Commercial Applications of the Research or Development:",
        "5": "The development of a dynamic, AI-driven knowledge graph for situational awareness has significant benefits across both defense and commercial sectors: 1.\nEnhanced Situational Awareness for Defense: The system will offer real-time integration of multimodal sensor inputs, improving decision-making for defense operators.\nBy providing an adaptive and interpretable knowledge graph, the system will enhance threat detection, reduce cognitive overload, and allow for faster, more accurate responses in critical defense scenarios.\nThis technology can be integrated into command centers, naval vessels, and air defense systems, improving the operational efficiency of combat teams by intelligently tracking and visualizing emerging threats.\n2.\nInteroperability and Scalability: The proposed framework\u2019s ability to fuse data from diverse sensor modalities ensures its adaptability across various military branches and systems, including those used by the Navy, Air Force, and ground units.\nIts scalable design means it can be expanded for use in larger, more complex operations, potentially supporting broader defense networks.\n3.\nCommercial Applications: Beyond defense, the technology has numerous commercial applications: \u2022 Disaster Response: The knowledge graph can be applied to real-time monitoring and coordination during natural disasters, enabling first responders to assess evolving situations, track assets, and optimize resource deployment.\n\u2022 Critical Infrastructure Protection: Industries such as energy, transportation, and water supply systems can use the system to monitor infrastructure, detect anomalies, and coordinate responses to potential threats or failures.\n\u2022 Autonomous Systems: In sectors like autonomous vehicles and robotics, the knowledge graph can enhance decision-making by integrating sensor data in real-time, allowing machines to navigate complex environments with improved situational awareness.\n4.\nMarket Potential: The system\u2019s ability to reduce operational latency while providing human-interpretable AI decisions positions it as a strong candidate for commercialization in fields that rely on rapid, data-driven decisions.\nThese include smart cities, security monitoring systems, and logistics, where understanding real-time data and responding to dynamic conditions are critical.\nBy combining AI transparency, real-time data fusion, and adaptive graph learning, the project has the potential to create impactful solutions across both defense and commercial markets, driving innovation in areas where situational awareness is paramount.\nAttention: Disclaimer: For any purpose other than to evaluate the proposal, this data except proposal cover sheets shall not be disclosed outside the Government and shall not be duplicated, used or disclosed in whole or in part, provided that if a contract is awarded to this proposer as a result of or in connection with the submission of this data, the Government shall have the right to duplicate, use or disclose the data to the extent provided in the funding agreement.\nThis restriction does not limit the Government's right to use information contained in the data if it is obtained from another source without restriction.\nThis restriction does not apply to routine handling of proposals for administrative purposes by Government support contractors.\nThe data subject to this restriction is contained on the pages of the proposal listed on the line below.",
        "6": "Addition: Enter the page numbers separated by a space of the pages in the proposal that are considered proprietary: List a maximum of 8 Key Words or phrases, separated by commas, that describe the Project: Dynamic Knowledge Graph, Situational Awareness, Multimodal Sensor Fusion, Threat Detection, Artificial Intelligence (AI), Real-time Decision-Making, Graph Neural Networks (GNNs), Semantic Fusion VOL I - Proposal Certification Summary: 1.\nAt a minimum, two thirds of the work in Phase I will be carried out by your small business as defined by 13 C.F.R YES Section 701-705.\nThe numbers for this certification are derived from the budget template.\nTo update these numbers, review and revise your budget data.\nIf the minimum percentage of work numbers are not met, then a letter of explanation or written approval from the funding officer is required.\nPlease note that some components will not accept any deviation from the Percentage of Work (POW) minimum requirements.\nPlease check your component instructions regarding the POW requirements.\nFirm POW Subcontractor POW 100% 0% 2.\nIs primary employment of the principal investigator with your firm as defined by 13 C.F.R Section 701-705?\nYES 3.\nDuring the performance of the contract, the research/research and development will be performed in the YES United States.\n4.\nDuring the performance of the contract, the research/research and development will be performed at the YES offerors facilities by the offerors employees except as otherwise indicated in the technical proposal.\n5.\nDo you plan to use Federal facilities, laboratories, or equipment?\n6.\nThe offeror understands and shall comply with export control regulations.\n7.\nThere will be ITAR/EAR data in this work and/or deliverables.\nNO YES YES 8.\nHas a proposal for essentially equivalent work been submitted to other US government agencies or DoD NO components?\n9.\nHas a contract been awarded for any of the proposals listed above?\n10.\nFirm will notify the Federal agency immediately if all or a portion of the work authorized and funded under this proposal is subsequently funded by another Federal agency.\nNO YES 11.\nAre you submitting assertions in accordance with DFARS 252.227-7017 Identification and assertions use, YES release, or disclosure restriction?\n12.\nAre you proposing research that utilizes human/animal subjects or a recombinant DNA as described in DoDI NO 3216.01, 32 C.F.R.\nSection 219, and National Institutes of Health Guidelines for Research Involving Recombinant DNA of the solicitation: 13.\nIn accordance with Federal Acquisition Regulation 4.2105, at the time of proposal submission, the required YES",
        "7": "certification template, \"Contractor Certification Regarding Provision of Prohibited Video Surveillance and Telecommunications Services and Equipment\" will be completed, signed by an authorized company official, and included in Volume V: Supporting Documents of this proposal.\nNOTE: Failure to complete and submit the required certifications as a part of the proposal submission process may be cause for rejection of the proposal submission without evaluation.\n14.\nAre teaming partners or subcontractors proposed?\nNO 15.\nAre you proposing to use foreign nationals as defined in 22 CFR 120.16 for work under the proposed effort?\nNO 16.\nWhat percentage of the principal investigators total time will be on the project?\n17.\nIs the principal investigator socially/economically disadvantaged?\n60% YES 18.\nDoes your firm allow for the release of its contact information to Economic Development Organizations?\nYES VOL I - Contact Information Principal Investigator Name: Mohsen Imani Phone: (619) 549-9084 Email: m.imani@ai-sensation.com Address: 26766 Ashford, MISSION VIEJO, CA 92692 - 0000 Corporate Official Name: Mohsen Imani Phone: (619) 549-9084 Email: m.imani@ai-sensation.com Address: 26766 Ashford, MISSION VIEJO, CA 92692 - 0000 Authorized Contract Negotiator Name: Haleh Alimohamadi Phone: (619) 549-9084 Email: cto@ai-sensation.com Address: 26766 Ashford, MISSION VIEJO, CA 92692 - 0000 Form Generated on 10/19/2024 23:33:36",
        "8": "AI-Generated Dynamic Knowledge Graph for Situational Awareness The key to our framework is the use of a large language model (LLM) to generate and maintain a dynamic knowledge graph as the central mechanism for enabling real-time situational awareness between human op- erators and AI systems.\nThis knowledge graph will serve as a shared, continuously evolving structure that facilitates interaction between human and machine actors.\nBy allowing real-time updates based on user inputs, mission data, and changing operational contexts, the knowledge graph becomes a powerful tool for manag- ing and processing complex hierarchical information in defense operations.\nIts dynamic nature ensures that it adapts seamlessly to the evolving environment, providing a scalable and interpretable framework through which situational awareness is maintained, decisions are made, and potential threats are continuously moni- tored.\nThis integration between human inputs, AI-driven reasoning, and the dynamic knowledge graph em- powers operators to effectively manage mission-critical scenarios in real time, a necessity in time-sensitive Air Force missions.\nDr.\nMohsen Imani, a faculty member at UC Irvine and President of AI-Sensation LLC, the team comprises over 50 members.\nDr.\nImani has a proven track record of transferring technologies to DARPA, the US Air Force, the US Army, and leading industries such as Cisco, Intel, and IBM.\nRecognized as a rising star in the DoD community, he has received numerous prestigious young faculty awards from DARPA, the US Navy, the US Army, and multiple industry leaders.\nA Introduction In modern defense operations, the ability to process, interpret, and act upon large-scale, multimodal data streams in real-time is essential to maintain situational awareness.\nDelays or inef\ufb01ciencies in handling this data can compromise mission success.\nTraditional AI-driven systems often rely on rigid models and prede\ufb01ned structures, limiting their \ufb02exibility and adaptability in dynamic, time-constrained environments typical of Air Force missions.\nAs these mis- sions become increasingly complex and time-sensitive, there is a growing need for systems that not only interpret data in real-time but also adapt continuously to evolving operational contexts.\nOur approach, which centers on a dynamic knowledge graph, addresses these needs by offering a \ufb02exible, scalable solution that continuously adapts to user input and mission-speci\ufb01c requirements.\nThe knowledge graph is the foundation of this framework, providing a structured representation of entities, events, and relationships critical to mission objectives, such as pattern-of-life analysis, threat detection, and targeting operations.\nBy dynamically updating the graph in response to real-time mission data and user interaction, the system ensures that situational awareness is maintained even as mission parameters change.\nThe dynamic nature of the graph allows it to evolve with operational conditions, ensuring it provides an accurate, mission-relevant representation of the environ- ment.\nThis is crucial for time-sensitive Air Force applications, where conditions can shift rapidly based on adversary tactics, environmental changes, or new intelligence.\nThe primary goal of our framework is to create a highly interactive dynamic knowledge graph that allows oper- ators to directly engage with the system, enabling real- time modi\ufb01cations and additions.\nAs the user interacts with the graph\u2014whether by adding new data points, re- \ufb01ning mission objectives, or introducing contextual in- formation\u2014the system will suggest updates to surround- ing nodes and edges, ensuring that the entire structure remains coherent and aligned with the evolving mission objectives.\nThese suggestions are informed by both user inputs and the AI\u2019s interpretation of mission data, allowing the system to automatically \ufb01ll data gaps, resolve con- \ufb02icting information, and optimize the overall graph structure to support decision-making under high-pressure, time- constrained conditions.\nHowever, the development and maintenance of such an interactive knowledge graph requires addressing two primary challenges: Fig.\n1: Illustration of the complex capabilities that our mission-speci\ufb01c AI should enable with limited data.\n\u2022 Dynamic knowledge graph generation for complex missions: Air Force missions often require more than sim- ple object detection or classi\ufb01cation.\nThese missions involve dynamic, multifaceted objectives that can change based on evolving conditions, such as shifts in adversary tactics, geopolitical environments, or new intelligence.",
        "9": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC For example, a mission directive like \u2018Detect any suspicious air activity\u2019 involves integrating data across multiple modalities\u2014such as geographical, temporal, and behavioral factors\u2014to identify potential threats.\nThe knowledge graph must re\ufb02ect these complexities and adjust dynamically as new inputs or changes occur, ensuring it provides an accurate, mission-relevant representation of the environment.\n\u2022 AI-driven processing and interpretation of the knowledge graph: Once the knowledge graph is constructed and dynamically updated, it must be processed in a manner that is transparent and interpretable by both human operators and AI systems.\nCurrent solutions like graph neural networks (GNNs) are effective for analyzing static graphs but fall short in handling dynamically evolving graphs and often lack transparency, functioning as black-box models.\nOur framework addresses these limitations by utilizing symbolic reasoning algorithms capable of processing the graph in a human-interpretable way.\nThis ensures that the system\u2019s reasoning is understandable, transparent, and trustworthy, a necessity in critical defense operations.\nBy integrating advanced AI technologies with the dynamic knowledge graph, our framework enables real-time human- AI collaboration in the decision-making process.\nThe graph acts as a centralized knowledge repository, organizing mission-speci\ufb01c data and facilitating continuous updates as new information becomes available.\nFor example, if tasked with identifying a potential threat to an airspace, the knowledge graph will not only store data on previously detected aircraft but will dynamically adapt to incorporate new indicators of suspicious behavior.\nAs the graph evolves, the system provides real-time feedback, suggesting updates to mission objectives based on emerging patterns or anomalies.\nWhile the knowledge graph is designed to be human-interpretable, its complexity may often surpass the capacity of manual analysis, particularly in high-pressure, time-constrained environments.\nTo address this, our framework inte- grates hyperdimensional cognitive computing (HDC) for reasoning over the graph\u2019s structure.\nHDC-based reasoning enables the system to process complex relationships within the knowledge graph ef\ufb01ciently, delivering insights that are both transparent and actionable for human operators.\nThis ensures that situational awareness is maintained at all times, even in rapidly evolving scenarios.\nAdditionally, the knowledge graph is capable of generalizing across complex mission scenarios, allowing it to handle tasks beyond typical detection and classi\ufb01cation.\nFor instance, detecting changes in airspace activity may involve monitoring not only the physical presence of objects but also their \ufb02ight patterns, operational behaviors, and potential deviations from expected norms.\nAs the mission evolves, so too will the knowledge graph, ensuring it remains aligned with current objectives and re\ufb02ects real-time operational requirements.\nOur ultimate objective is to create a dynamic knowledge graph framework that allows for seamless collaboration be- tween human operators and AI systems.\nBy preserving mission-speci\ufb01c semantic knowledge and continuously adapt- ing to real-time data, this framework ensures situational awareness is maintained, operational ef\ufb01ciency is maximized, and human operators are equipped with actionable, interpretable insights.\nIn doing so, our framework addresses the critical need for adaptable, human-interpretable systems in modern defense operations.\nB Project Overview This proposal aims to develop an innova- tive AI framework that emphasizes three crit- ical features: context awareness, dynamic knowledge graph management, and opera- tional ef\ufb01ciency.\nFigure 2 illustrates our framework, which interprets and processes complex, multimodal data streams to detect a ).\nThis user-de\ufb01ned defense missions ( AI framework utilizes large language models (LLMs) to generate a dynamic knowledge graph that abstracts complex operational sce- narios into structured, symbolic formats.\nBy doing so, it allows for real-time updates and encapsulates the intricacies of defense mis- sions in an actionable manner for human op- b ).\nThe knowledge graph dynami- erators ( cally represents mission-critical information, ensuring that all relevant data is coherently structured across the graph\u2019s nodes.\n\u2022 \u2022 Fig.\n2: Our framework enables dynamic knowledge graph AI for gen- eralizing complex defense missions: (a) Users can de\ufb01ne any desired target.\n(b) Our framework leverages LLMs to translate user input into semantic graphs.\n(c) Multi-modal data is matched with the graph us- (d) Reasoning over the graph identi- ing joint embedding methods.\n\ufb01es mission-relevant data.\n(e) The graph and reasoning method can be adapted to low-SWaP systems.\n2",
        "10": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC c ).\n\u2022 \u2022 \u2022 LLMs in our framework are used to automate the construction of mission-speci\ufb01c knowledge graphs but do not serve as the decision-makers.\nInstead, they act as experts that abstract user-de\ufb01ned missions into dynamic, interpretable, and traceable knowledge, allowing for mission generalization even with limited data ( Unlike traditional AI models that operate over static data, our approach leverages joint embedding models to map incoming multimodal data into the semantic space of the dynamic knowledge graph.\nThe system performs spatial- d ).\nFurthermore, the dynamic knowledge temporal reasoning to determine whether mission-relevant data is detected ( graph is designed for ef\ufb01ciency.\nOnce the graph has been generated, it can be reused and updated in real-time without requiring LLM intervention, signi\ufb01cantly simplifying inference by combining joint embedding and symbolic reasoning e ).\nThis ensures our AI model maintains operational complexity while offering low Size, Weight, and approaches ( Power (SWaP) capabilities, making it adaptable for defense applications where resource ef\ufb01ciency is crucial.\nWe aim to establish an infrastructure that enables defense operators to de\ufb01ne complex mission objectives that are translated into symbolic, contextual knowledge graphs.\nThis abstraction aids in the comprehension and execution of complex activities and adapts the AI model to handle evolving mission-speci\ufb01c tasks.\nThe dynamic knowledge graph is designed to predict mission-related events early, offering timely warnings and signi\ufb01cantly enhancing situa- tional awareness.\nAdditionally, we plan to distill knowledge from large AI models into a compact, hyperdimensional contextual model.\nThe resulting model will ensure transparent and trustworthy decision-making, particularly within resource-constrained environments, bridging the gap between large-scale AI capabilities and the practical limitations of smaller defense systems.\nBy integrating these advanced features, our framework aligns with the core principles of real-time processing, adaptability, and ef\ufb01cient resource use, ensuring robust performance in defense operations under time-sensitive conditions.\nInterpretability and Traceability: The transparency of our framework offers signi\ufb01cant bene\ufb01ts for managing complex signal processing systems.\nEach node\u2019s contex- tual knowledge base not only details all mission-relevant scenarios but does so in a locally interpretable man- ner, enabling human-machine interactions and decision- making processes.\nThe model\u2019s transparency serves dual purposes: providing a human-level explanation for each prediction and allowing operators to modify the knowl- edge directly, thus enhancing the model\u2019s accuracy and transparency.\nFigure 3 shows an overview of our proposed AI framework, describing the signal processing activities within a uni\ufb01ed, contextual level understandable by human operators.\nThis structured approach not only facilitates more accurate predictions but also enhances the interpretabil- ity of these predictions, ensuring that AI-generated insights are both reliable and relatable to human operators.\nBy integrating these advanced capabilities, our framework seeks to revolutionize the monitoring and management of data streams, ensuring greater operational ef\ufb01ciency and effectiveness in alignment with the objectives of the SBIR call.\nTo further enhance the transparency and reliability of our AI framework, we implement comprehensive methodologies aimed at identifying and mitigating AI risks by systematically quantifying and adjusting the level of human oversight versus automation throughout the model\u2019s lifecycle.\nThese methodologies are integrated into every phase of model development, training, testing, and deployment, ensuring that the AI system remains aligned with operational goals and safety standards.\nIn the development phase, we employ a risk assessment approach that involves continuous monitoring of the AI\u2019s decision-making processes.\nThis includes quantifying the extent of human involvement required at various stages, allowing for dynamic adjustments to the level of automation.\nFor instance, during critical mission scenarios, the system may increase human oversight, ensuring that operators can intervene or validate AI decisions in real time.\nConversely, in routine or well-understood tasks, the system can operate with higher levels of automation, thus optimizing ef\ufb01ciency without compromising safety.\nDuring training and testing, our methodologies focus on establishing clear thresholds for when human intervention is necessary.\nBy embedding human-in-the-loop (HITL) mechanisms, we ensure that the AI model\u2019s predictions and decisions can be reviewed and adjusted by human operators.\nThis process is critical for \ufb01ne-tuning the model\u2019s performance and preventing the system from operating outside its intended parameters.\nAdditionally, we incorporate stress testing under various operational scenarios to evaluate the model\u2019s behavior in edge cases, further informing the balance between human and automated control.\nFig.\n3: The proposed semantic AI describes the mission using the same uni\ufb01ed language as a human operator.\n3",
        "11": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC \u2022 \u2022 C Details of Semantic AI Framework As Figure 4 shows, we aim to implement an AI framework that learns generalized concepts instead of recognizing prede\ufb01ned patterns.\nOur strategy is to design a zero-shot learning system where users can input the mission.\nThese phrases will be deconstructed into relevant words and attributes, which build a detailed contextual graph.\nThis symbolic knowledge maintains the semantics of all scenarios where the mission of interest may occur.\nThis graph can be built in zero-shot using advanced machine learning models and preserves the abstraction required for generalizing the mission.\nThe next step of our framework is to match the semantic graph with data using a joint embedding model.\nThe joint embedding quanti\ufb01es the relevance of each frame to the associated words in the semantic graph.\nAs shown in Figure 4, the process involves several key steps: 1 User-De\ufb01ned Mission: Users can de\ufb01ne com- plex scenarios of interest as missions.\nFor instance, they might input phrases like detect any potential threat to naval systems, potential strike, or suspi- cious air activity.\nAll these phrases describe com- plex missions that are hard to generalize.\nIn this whitepaper, we describe the mission as potential hostage situation, as it is easier to interpret.\n2 Semantic Graph Generation: Our framework passes the phrase of interest to a machine learn- ing model to generate a semantic graph related to the topic of interest.\nThis graph captures the intri- cate relationships and context-speci\ufb01c details rele- vant to the mission.\nThe more detailed and speci\ufb01c mission will result in the generation of a smaller semantic graph.\n3 Joint Embedding: The generated semantic graph is then matched with incoming data using joint-embedding transformers.\nThis step ensures that the contextual relevance of each piece of data is accurately quanti\ufb01ed.\n4 Af\ufb01nity Matrix Creation: The match results are presented as an af\ufb01nity matrix, which quanti\ufb01es the relevance of each word in the semantic graph to speci\ufb01c frames of incoming data.\nThis matrix helps in identifying which parts of the data are most relevant to the de\ufb01ned mission.\n5 Spatial-Temporal Reasoning: Finally, spatial-temporal analogical reasoning is applied over the match semantics in the af\ufb01nity matrix to detect the mission.\nThis reasoning process allows the system to understand the context and temporal sequence of events, improving the accuracy of detecting complex scenarios.\nWe exploit the power of hyperdimensional computing mathematics to enable reasoning over large-scale semantic graphs with extremely small numbers of data [4, 8].\nBy analyzing patterns over time and across different data sources, our reasoning framework can provide high-con\ufb01dence detections of critical in- cidents.\nThis comprehensive approach ensures that the AI framework can respond to new and evolving environments without requiring extensive retraining or data pre-labeling, thus enhancing systems\u2019 resilience and responsiveness.\nFig.\n4: Overview of our framework for generalizing complex missions: (1) Users can de\ufb01ne any desired task/mission.\n(2) Our framework passes the target phrase into a large language model to generate a semantic graph abstracting the mission.\n(3) We use joint-embedding transformers to match every node in the seman- tic graph with incoming data.\n(4) The match results are presented as an af\ufb01nity matrix, quantifying the relevance of each word.\n(5) Spatial-temporal reasoning is applied over the match semantics in the af\ufb01nity matrix to detect the mission.\n\u2022 \u2022 \u2022 C.1 Semantic Graph Generation and Dynamic Update Figure 5 describes how our framework builds the semantic graph and how it leverages it for analogical guidance.\nThis graph acts as a benchmark for making inferences, enabling the system to make decisions that are not only well-informed but also consistent and reliable.\nWhen the system processes new input data, it conducts a two-part analysis.\nFirst, it interprets the data to create a seman- tic description of the current data, capturing the details and context of the surroundings.\nThen, this semantic overview is compared with the established knowledge using graph- Fig.\n5: Example of genereted knowledge graph for complex tasks.\n4",
        "12": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC matching algorithms.\nThe decision-making process that results from this comparison will be fully interpretable.\nUnlike the often complex processes associated with deep neural networks, where the reasons behind a decision are hidden within many layers of neural activity, the logic in our symbolic reasoning model\u2019s decisions is clear and human- understandable.\nEach decision can be traced to a logical sequence of pattern matches and recognized scenarios, similar to how a human expert might explain their reasoning.\nThis clarity in how decisions are made greatly increases the system\u2019s trustworthiness.\nUsers can easily understand and verify why each decision was made, which builds con\ufb01dence.\nIn the critical context of DoD operations, where each decision can have major consequences, this trust is essential.\nFurthermore, the transparency inherent in our framework improves safety.\nBy being able to predict the system\u2019s responses to various situations and verify the logic behind them, operators can ensure that the system behaves consis- tently within safe boundaries.\nOur innovative framework is designed to generate semantic graphs dynamically based on its interactions with the environment, providing a robust and adaptable solution for multimodal information pro- cessing.\nInitially, the semantic graph is generated in a zero-shot manner, without any prior access to actual semantic graphs.\nThis capability allows the framework to abstract the desired mission as de\ufb01ned by the user, capturing the essence of complex scenarios.\nAt the outset, our models employed by our framework interpret user-de\ufb01ned missions and generate relevant semantic keywords and relationships.\nThese initial semantic graphs serve as the foundational structure for understanding and detecting anomalies or de\ufb01ned missions.\nC.2 Retrieval-Augmented Generation for Semantic Enhancement & Interpetability To achieve a fully transparent and interpretable AI framework, particularly in the context of complex and mission- critical defense applications, it is essential to address the inherent challenges posed by semantic generation using Large Language Models.\nWhile LLMs are powerful tools for generating semantics, they often operate as black boxes, producing outputs that can be dif\ufb01cult to trace or understand.\nThis opacity can be a signi\ufb01cant drawback in defense scenarios where decision-making must be both transparent and justi\ufb01able.\nTo bridge this gap, we propose the use of Graph Retrieval-Augmented Generation (Graph RAG) [5], which enhances the LLM\u2019s ability to generate semantic graphs while maintaining full traceability to the source documents.\nThis ensures that every decision made by the AI can be traced back through a clear and interpretable hierarchy, thereby addressing the critical need for transparency and accountability.\nGraph RAG not only improves transparency but also offers substantial enhancements in semantic generation quality.\nBy retrieving and integrating relevant contextual information from a curated set of documents, Graph RAG re\ufb01nes the LLM\u2019s outputs, leading to more accurate and contextually relevant semantic graphs.\nThis is particularly valuable in defense applications, where accurate generalization from limited data is crucial.\nThe ability of Graph RAG to generate high-quality semantics with fewer samples aligns directly with the Army\u2019s AI/ML Focused Open Topic, which em- phasizes the need for scalable AI techniques that can function effectively in data-scarce environments.\nFurthermore, the transparency and traceability provided by Graph RAG ensure that the AI framework not only meets the operational demands of advanced computing but also adheres to the principles of Trusted AI and Autonomy, as outlined in the call.\nBy integrating Graph RAG, the proposed framework is positioned to deliver both the technical performance and the interpretability required for next-generation defense AI systems.\nC.3 Early Prediction of Complex Scenarios Early detection and prediction of complex scenarios are critical for enhancing defense capabilities by providing actionable situational awareness.\nOur proposed frame- work leverages a dynamic knowledge graph, continu- ously updated with real-time data and contextual infor- mation, to predict emerging scenarios.\nThe strength of our approach lies in its ability to anticipate complex sce- narios early, allowing defense systems to take preemp- tive actions and mitigate risks before situations escalate.\nTo demonstrate this capability, we developed an initial implementation of our dynamic knowledge graph-based AI system for anomaly detection using RF signals.\nFigure 6 illustrates how our framework, through continuous updates of the knowledge graph, builds con\ufb01dence early in the timeline, well before the actual event occurs.\nThis dynamic graph continuously incorporates real-time anomalies detected in RF signal patterns.\nWhen unusual RF signals are detected, the knowledge graph is updated with new rela- tionships, re\ufb02ecting the evolving scenario.\nThe model\u2019s con\ufb01dence grows as new data points strengthen the anomaly\u2019s Fig.\n6: Semantic AI Con\ufb01dence in early prediction.\n5",
        "13": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC likelihood, with the framework predicting with over 95% con\ufb01dence long before the anomaly fully develops, providing early warnings to decision-makers.\nThe dynamic nature of the knowledge graph enables early predictions in various defense scenarios, from RF signal anomalies to complex mission threats.\nThe timeframe for detection is context-dependent, but in scenarios involving sudden changes, such as signal strength spikes or adversarial movements, early detection can occur within seconds.\nThis proactive approach allows defense systems to take precautionary measures, reducing potential damage and mis- sion disruption.\nOur framework\u2019s early prediction capability has been validated in different contexts, including detect- ing potential threats before they fully materialize.\nWe plan to rigorously test this framework for predicting threats in defense systems equipped with our knowledge graph-based reasoning model.\nBy providing early predictions, war\ufb01ght- ers and autonomous systems gain the ability to make timely decisions, improving the likelihood of neutralizing threats before signi\ufb01cant damage occurs.\nC.4 Dynamic Knowledge Graph Contribution to Early Prediction The early prediction capability of our framework is fundamentally driven by the dynamic nature of the knowledge graph.\nUnlike static models, our system continuously updates its knowledge graph as new data \ufb02ows in.\nThis allows the AI to reason over time, integrating temporal and contextual elements to predict scenarios as they evolve.\nFor example, when early signs of anomalous behavior are detected, the knowledge graph adjusts by adding new edges and nodes to represent the evolving situation.\nThis reasoning process is further enhanced by Hyperdimensional Cognitive Computing (HDC), which enables analogical reasoning over the graph.\nBy binding new data with existing entities and relationships in the knowledge graph, the AI can detect early warning signs, calculate con\ufb01dence levels, and alert operators before the full development of a threat.\nHDC ensures that the system can quickly compare new patterns with historical knowledge, leading to faster and more reliable predictions.\nThis early prediction capability, powered by the dynamic knowledge graph, positions our framework as an essential tool for defense applications, offering proactive responses to emerging threats in real-time operational contexts.\nC.5 Dynamic Knowledge Graph through Human-AI Interaction Our framework leverages a dynamic knowl- edge graph that evolves through continuous interaction between human operators and AI systems.\nThis knowledge graph enables situ- ational awareness by dynamically capturing, structuring, and interpreting complex rela- tionships in the operational environment.\nA key innovation in our approach is the use of large language models (LLMs) as an inter- face, allowing AI and human users to col- laboratively build a shared knowledge base.\nThis interaction enhances real-time decision- making and control over evolving mission objectives.\nIn complex and dynamic environments, timely and accurate decision-making is cru- cial.\nHuman operators often face challenges in making quick decisions without clear, real-time recommendations.\nOur dynamic knowledge graph framework, powered by semantic AI, addresses these challenges by offering a transparent and robust decision-making model that adapts to operational contexts.\nThe system continuously updates the knowledge graph based on real-time inputs, providing actionable recommendations and decisions.\nThis approach facilitates human-AI collaboration, enhancing situational awareness and enabling operators to make informed, context-aware decisions.\nThe framework enhances both autonomous systems and human operators by providing transparent, interpretable decision-making capabilities.\nFigure 7a illustrates the different methods by which the AI system interacts with human operators.\nBased on the evolving knowledge graph, the system delivers feedback in three primary forms: visual- izations, real-time recommendations, and automated decision-making.\nThese interactions enable operators to make complex decisions ef\ufb01ciently, in alignment with mission-speci\ufb01c requirements.\nFig.\n7: (a) Our semantic AI enables various methods of visualizing graphical and verbal communications with human operators and facili- tates symbolic decision-making.\n(b) AI-based decision-making in com- plex scenarios.\nThe decisions aim to enhance situational awareness for operators and systems, enabling timely decisions or recommendations in critical situations.\n1.\nVisualization and Prediction: The AI system visualizes and predicts potential outcomes or actions of targets using the dynamic knowledge graph.\nIt can trace matched semantics back to entities or objects in the knowledge graph, 6",
        "14": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC allowing operators to visualize mission-relevant objects or behaviors in a scenario.\nFor example, the semantic AI framework can highlight potential threats in video feeds, helping operators take proactive measures.\n2.\nReal-Time Recommendations: The system provides verbal or written descriptions based on the current state of the knowledge graph, offering human operators a deeper understanding of the situation.\nThese recommendations enhance situational awareness by suggesting concrete actions based on contextual data.\nFor instance, in scenarios involving signal interference, the AI might recommend avoiding certain frequencies or taking countermeasures.\nThe recommendations, derived from the dynamic knowledge graph, remain interpretable and aligned with mission- speci\ufb01c objectives, enabling informed, real-time decisions.\n3.\nAutomated Decision-Making: Our framework autonomously generates a semantically-enhanced scene graph, where nodes represent mission-relevant entities and edges represent relationships derived from the knowledge graph.\nNeuro-symbolic AI algorithms process this scene graph, enabling the AI to make symbolic and inter- pretable decisions.\nIn time-sensitive scenarios, where rapid response is critical, the AI can autonomously make decisions such as activating defensive measures or rerouting resources.\nThese decisions are fully transparent, with justi\ufb01cations provided by the knowledge graph, building trust between human operators and autonomous systems.\nThe symbolic nature of our framework enables decisions that are both transparent and traceable.\nThe AI constructs a decision-making graph that optimally represents the best possible outcomes for given scenarios.\nEach decision is embedded in the knowledge graph and processed by symbolic reasoning models, ensuring that operators can track and understand every decision the system makes.\nThis symbolic reasoning ensures that decisions are not only accurate but also explainable, which is essential for defense applications.\nUnlike traditional black-box AI models, which absorb vast amounts of knowledge and provide opaque decisions, our framework focuses on mission-speci\ufb01c information.\nThe dynamic knowledge graph limits its scope to current mission objectives, ensuring the system remains ef\ufb01cient and focused.\nThis enables decision-making capabilities to be deployed on resource-constrained devices, such as edge computing systems or Tiny AI platforms, making real-time decision-making feasible even in low-power environments.\nC.5.1 Semantic Knowledge Aggregation Across Heterogeneous Devices Figure 8 illustrates our framework for semantic knowledge aggregation, which enables seamless human-machine and machine- machine collaboration in complex, mission- critical scenarios.\nMultiple devices equipped with different sensors and AI models ana- lyze the environment from varied perspec- tives.\nThis heterogeneous setup poses chal- lenges for aggregating and sharing knowl- edge across the network to support collab- orative decision-making.\nOur framework abstracts the knowledge gen- erated by each device, regardless of its sen- sor type or AI model, into a uni\ufb01ed dy- namic knowledge graph.\nEach device oper- ates within the mission-speci\ufb01c framework, updating its own knowledge graph by generating semantic representations of mission-relevant data.\nThese semantic abstractions enable devices to perform analogical reasoning and predictive capabilities.\nFor instance, if a device detects data aligning with a pre-constructed semantic representation of a \u201dhostage situation,\u201d it triggers response actions, updating the shared knowledge graph to re\ufb02ect new information.\nThe strength of our approach lies in the uni\ufb01ed space for dynamic knowledge aggregation, where the semantic knowl- edge from multiple devices is synthesized into a single, comprehensive graph.\nThis uni\ufb01ed graph integrates data from heterogeneous sources, providing a shared decision-making tool accessible to autonomous systems and human opera- tors.\nBy representing knowledge symbolically, the framework ensures information is interpretable by both machines and humans, enhancing situational awareness and informed decision-making in real time.\nFig.\n8: Symbolic Knowledge aggregation across devices with hetero- geneous sensors, resolutions, and points of observation.\nMachine-Machine and Human-Machine Collaboration: The dynamic knowledge graph serves as the central repository where device insights are continuously aggregated and updated, allowing for real-time collaboration.\nWhen one device detects an anomaly or mission-relevant event, the knowledge graph updates, triggering actions on other 7",
        "15": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC devices or notifying human operators.\nThis collaboration is made possible through a uni\ufb01ed semantic space, where data from multiple heterogeneous sensors is abstracted and linked via the dynamic knowledge graph.\nThe system also enhances human-machine collaboration.\nAI presents aggregated knowledge to human operators in a transparent manner, allowing operators to visualize and interact with mission-critical data in real time.\nThis facilitates collaborative decision-making, combining human reasoning with machine intelligence.\nContextual Knowledge Fusion Across Devices: A key advantage of our framework is its ability to fuse knowledge across devices with varying sensor types, resolutions, and observational capabilities.\nThe knowledge graph continu- ously updates, integrating different perspectives to create a holistic view of the situation.\nFor example, devices with visual sensors may detect object movements, while RF sensors might identify anomalies in communication signals.\nThese data streams are abstracted into the dynamic knowledge graph, allowing the AI to reason over multimodal data, combine temporal and spatial insights, and provide mission-critical recommendations.\nThis approach enables knowledge fusion for generalizing complex defense missions, making the framework adaptable to various operational contexts.\nThe dynamic knowledge graph enables real-time information sharing, enhancing decision-making and operational effectiveness in defense missions, even in resource-constrained environments.\nWe aim to develop a framework that enables knowl- edge aggregation from robots equipped with diverse sen- sors and AI models, even in complex tasks.\nOur ap- proach utilizes LLMs to transform complex scenar- ios, such as hostage situ- into a knowledge ations, This abstraction graph.\nprocess converts the scene into a graph structure, in- dependent of sensor type or \ufb01eld of view.\nThese graphs, representing scenarios like hostage situations, remain consistent across different sensor data, including images and LiDAR.\nFigure 9 shows the functionality of our framework, abstracting knowledge of each robot equipped with heterogeneous sensors.\nEach robot will utilize our framework to construct a graph for analogical reasoning and future predictions.\nFor instance, new data resembling the abstract knowledge of a \u2019hostage situation\u2019 stored in each node can lead to such identi\ufb01cation.\nCrucially, since these abstractions reside in a uni\ufb01ed knowl- edge space, they can be merged into a universal model representing the \u2019pattern of life\u2019.\nThis means knowledge from robots with heterogeneous sensors and varying perspectives can be amalgamated into shared, comprehensive decision- making.\nOur framework offers transparent, interpretable knowledge in the form of symbolic graphs, detailing critical scenarios.\nThis feature enables clear, locally interpretable decisions, enhancing resilience against adversarial actions and noise \u2014 crucial for DoD missions in potentially malicious and noisy environments.\nFig.\n9: Symbolic knowledge representation, abstraction of complex scenario, and Knowl- edge aggregation across multiple sensors.\nD Air Force Applications: Reasoning-Driven AI Capabilities D.1 Reasoning-Guided Prediction via Enhanced LLM [3, 13] Exploiting the knowledge graph, we present a two-stage task-oriented learning algorithm that tackles challenges in prior algorithms all at once, motivated by the recent success of large-scale Vision-Language Models (VLMs).\nIn the \ufb01rst stage, we exploit LLM to generate a dynamic and detailed knowledge graph.\nFigure 10 shows an overview of the proposed framework.\nOur system begins with the user de\ufb01ning a task that requires the identi\ufb01cation of speci\ufb01c objects or items ( 1 ).\nFor instance, a user might seek objects that can remove a lemon from a tea glass, ranging from a knife to a tea strainer or fork.\nOur framework prompts LLMs to generate a list of items commonly associated with the task ( 2 ).\nHowever, the actual objects present in the input data may vary widely, including instances where the desired objects are obscured or presented in a form dif\ufb01cult to recognize.\nThis limitation underscores the need for a generalization that is not constrained to objects explicitly listed by LLMs.\nTo achieve this, we introduce a method that requests LLMs to also identify common features of the listed items that facilitate the task\u2019s completion ( 3 ).\nFor example, a fork might be described as \u201chard,\u201d \u201csharp,\u201d and having a \u201clong handle.\u201d Our framework then constructs a feature graph for each item, enabling the identi\ufb01cation of other objects with similar characteristics, even if they were \u2022 \u2022 \u2022 8",
        "16": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC \u2022 not speci\ufb01cally mentioned by the LLM.\nThis method allows for the recognition of alternative objects, such as spoons or pens, which, while not listed by LLMs, possess the requisite features for the task.\nOur sys- tem employs image segmentation to iden- tify distinct items or regions within the in- put data ( 4 ).\nEach segment is then ana- lyzed by a joint embedding model, such as ImageBind or OpenCLIP, to match it with the previously generated feature list ( 5 ).\nThis process creates an af\ufb01nity matrix, indicating which segments likely contain objects with the desired features ( 6 ).\nFor instance, both a plastic spoon and fork might match the features iden- ti\ufb01ed for task completion.\nThese joint embedding models, trained across multi- ple modalities, ensure that textual and vi- sual data converge on similar embeddings, thereby facilitating accurate object identi\ufb01cation.\n\u2022 \u2022 Fig.\n10: Task-based object detection: (1) users de\ufb01ne their complex task or mission where objects are to be detected, (2) LLMs analyze the task and generate a list of objects that could perform the desired task, (3) for generalization, a set of features will be generated for each object, (4) in- put image will be segmented, (5) each segment will be fed into a joint embedding along with the features generated by LLMs to \ufb01nd matching, (6) the segments with highest similarity will be selected as top items ca- pable of performing the desired task.\nD.2 Air Force Capabilities Modern air defense systems face in- creasingly complex challenges as ad- versaries leverage advanced technolo- gies such as camou\ufb02age, decoys, and system modi\ufb01cations to evade detec- tion.\nThese evolving threats demand AI capabilities that extend beyond con- ventional pattern recognition and static models.\nCurrent AI models, often limited by \ufb01rst-order feature analysis, struggle to detect nuanced or adap- tive threats, leading to signi\ufb01cant hu- man intervention in critical decision- making scenarios.\nThis limitation highlights the urgent need for adapt- able AI systems capable of reasoning over complex, dynamic environments to enhance situational awareness, au- tonomy, and decision-making in Air Force missions.\nOur reasoning-driven AI framework directly addresses these challenges by leveraging dynamic knowledge graphs and deep reasoning capabilities.\nUnlike traditional AI systems that rely on static models and shallow feature detection, our framework integrates second- and third-order feature analysis, enabling it to identify camou\ufb02aged, disguised, or evolving threats.\nThe dynamic knowledge graph continuously updates based on real-time inputs, allowing the system to re\ufb01ne its understanding of the operational environment and enhance its decision-making autonomy in Air Force missions.\nFor instance, a camou\ufb02aged tank may evade detection by conventional AI systems that focus on super\ufb01cial features like shape or color.\nHowever, our reasoning-driven AI can analyze deeper features such as weight distribution, material composition, and movement patterns, identifying the tank as a threat even when primary features are concealed.\nSimilarly, the system can reason that an innocuous-looking object, such as a hammer, may be used as a weapon based on contextual information like material properties and situational cues.\nOur framework is optimized for low-SWaP (Size, Weight, and Power) applications, making it suitable for deployment on edge devices in Air Force missions.\nDespite tight power constraints, our reasoning-driven AI delivers 50% higher detection quality compared to conventional models, while operating with less than 5 watts of power.\nMoreover, the Fig.\n11: Application of Reasoning-driven AI for Air Force Intelligence.\n9",
        "17": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC Fig.\n12: Reasoning-driven AI for post-engagement threat assessment.\nsystem is highly adaptable, requiring minimal data to learn and reason in dynamic environments where large datasets may be unavailable.\nBelow are several key applications of our reasoning-driven AI framework in Air Force operations: Threat Assessment: Accurate threat assessment of adversary equipment, such as drones or aircraft, involves reason- ing over complex, evolving features.\nTraditional AI models struggle with variations in equipment con\ufb01gurations, such as different payloads, camou\ufb02age, or structural modi\ufb01cations, which can affect threat detection.\nOur reasoning-driven AI evaluates a broad range of features\u2014including payload type, missile range, and camou\ufb02age patterns\u2014by leverag- ing the dynamic knowledge graph.\nThis enables the system to assess threat levels accurately and provide real-time situational awareness to operators, allowing them to prioritize high-risk targets.\nFor example, our AI can analyze an aircraft\u2019s design, payload capacity, and range to assess its threat level and recommend optimal engagement strategies.\nPost-Engagement Assessment: After an engagement, it is crit- ical to assess whether a target has been neutralized to deter- mine if additional resources are needed.\nThis requires more than visual con\ufb01rmation of damage; it involves deep reasoning about integrity the target\u2019s structural and operational status.\nFor ex- ample, when evaluating a neu- tralized aircraft, our reasoning- driven AI assesses features such as engine functionality, control systems, and the likelihood of pilot incapacitation.\nThe dynamic knowledge graph is continuously updated with post-engagement data, allowing the system to reason in real time and provide actionable assessments to operators.\nThis ensures ef\ufb01cient resource allocation and mission success by focusing efforts on the highest remaining threats.\nDetecting Camou\ufb02aged or Modi\ufb01ed Equipment: Camou\ufb02age and equipment modi\ufb01cations are designed to deceive conventional detection systems.\nOur framework, however, applies deep feature analysis and reasoning over the dy- namic knowledge graph to identify camou\ufb02aged or modi\ufb01ed military assets.\nFor instance, a military tank hidden with foliage may evade detection based on visual appearance alone.\nHowever, by analyzing deeper features such as move- ment patterns, weight distribution, and heat signatures, our reasoning-driven AI can accurately detect the camou\ufb02aged asset.\nThis reasoning capability provides interpretable insights to human operators, enhancing situational awareness and reducing the likelihood of missing enemy assets.\nDetecting Decoys: Decoys are commonly employed to mislead adversaries and waste resources.\nConven- tional AI systems struggle with decoy detection be- cause they rely heavily on visual appearances.\nOur reasoning-driven AI, however, detects decoys by reason- ing over higher-order features, such as material composi- tion, structural inconsistencies, and movement patterns, revealing the true nature of the decoy.\nFor example, a decoy aircraft might mimic the external appearance of a real aircraft, but subtle differences in movement or struc- tural rigidity could be detected by our system.\nAddi- tionally, the framework\u2019s reasoning capabilities extend to identifying deepfakes or AI-generated data by analyz- ing inconsistencies in behavior or context, further enhancing its utility in cyber and information warfare.\nResource Optimization and Autonomy: A key advantage of our reasoning-driven AI framework is its ability to provide decision-making capabilities that are both precise and ef\ufb01cient in resource-constrained environments.\nThe system operates ef\ufb01ciently on edge devices, using less than 5 watts of power while delivering high-quality detection and reasoning.\nBy continuously updating the dynamic knowledge graph, the AI adapts to emerging threats in real time and autonomously optimizes resource allocation.\nThis ensures that Air Force missions maintain operational effectiveness without sacri\ufb01cing energy ef\ufb01ciency or performance in low-SWaP environments.\nIn summary, our reasoning-driven AI framework, powered by dynamic knowledge graphs, offers enhanced situational Fig.\n13: Reasoning-driven AI for camou\ufb02aged and decoy detection.\n10",
        "18": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC awareness, threat detection, and decision-making autonomy for complex Air Force operations.\nBy combining deep feature analysis with human-interpretable reasoning, the system ensures that sophisticated threats are effectively de- tected and neutralized, while maintaining high ef\ufb01ciency in resource-constrained environments.\nE Technical Details Fig.\n14: Overview of our ef\ufb01cient, symbolic, and interpretable fu- sion, learning, and reasoning framework.\nE.1 Hyperdimensional Cognitive Reasoning In this proposal, we plan to develop neuro- symbolic AI methods that assist in dynamically updating knowledge graphs for robust and ef- \ufb01cient information processing and transparent data fusion.\nThis approach will enable real-time situational awareness by using AI to process, reason, and learn from dynamically evolving op- erational environments, which is critical for de- fense operations that demand timely decisions under changing mission parameters.\nOur framework, based on Hyperdimensional Computing (HDC), seamlessly integrates symbolic reasoning with neural computation.\nHDC will serve as the backbone for constructing, updating, and reasoning over a dynamic knowledge graph in real-time, ensuring \ufb02exibility, scalability, and responsiveness to new mission inputs and changes in the oper- ational environment.\nThis dynamic knowledge graph will act as the central structure, evolving in response to real-time user inputs, mission data, and environmental conditions.\nBy embedding a cognitive reasoning layer within the AI, we ensure that the knowledge graph remains actionable, interpretable, and reliable for defense applications.\nThis aligns with the SBIR call\u2019s focus on enabling human-AI collaboration and providing transparent, real-time decision-making support.\nFigure 14 presents an overview of our proposed HDC framework, which supports several critical thrusts: \u2022 Knowledge Representation: Our framework enables a dynamic knowledge representation that encodes evolving mission objectives and contextual data into a high-dimensional space.\nThis encoding captures relationships between various data sources and mission-critical inputs, enabling continuous and meaningful updates to the knowledge graph.\nAs new data streams into the system, HDC ensures that both historical and new information are represented coherently, preserving critical mission knowledge while dynamically adapting to changes in real-time.\nThis capa- bility allows for the integration of multimodal data sources into a uni\ufb01ed knowledge structure, enhancing the AI\u2019s ability to reason effectively across diverse data types, which is essential for complex defense missions.\n\u2022 Real-Time Reasoning and Decision Making: HDC empowers the system to reason over the dynamically updated knowledge graph, enabling transparent and interpretable decision-making processes.\nBy leveraging symbolic rea- soning algorithms, the system can interpret complex mission-speci\ufb01c information and provide real-time recommen- dations and situational awareness insights to human operators.\nAs user inputs or environmental factors change, HDC-based reasoning will update and reinterpret the relationships between different entities within the knowledge graph, ensuring that decisions remain aligned with mission-critical objectives and responsive to the operational environment.\nThe combination of AI-driven updates and hyperdimensional cognitive reasoning ensures that the knowledge graph continuously re\ufb02ects the current situational context.\nThis allows both human operators and AI systems to maintain a shared understanding of evolving mission objectives, a critical capability that meets the Air Force\u2019s requirements for adaptive and trusted AI systems in real-time operational environments.\nDynamic Knowledge Graph Interaction: HDC operates through several well-de\ufb01ned cognitive operations that assist in dynamically updating and maintaining the knowledge graph, enabling the system to respond ef\ufb01ciently to real-time changes in the operational environment: \u2022 Binding (*): This operation binds two high-dimensional hypervectors, representing different entities, into a new hypervector that captures the association between these entities.\nThis is crucial for dynamically representing rela- tionships between mission objectives and evolving contextual data within the knowledge graph.\n\u2022 Bundling (+): The bundling operation aggregates multiple hypervectors into a single vector, representing a set of related concepts.\nThis operation is used to fuse diverse data sources into the knowledge graph, ensuring that multi- modal data streams are captured holistically and interpreted in context.\n\u2022 Permutation (\u03c1): Permutation allows the system to capture sequences and temporal relationships within the knowl- edge graph.\nThis operation enables the AI to reason about temporal patterns, such as identifying a sequence of 11",
        "19": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC Fig.\n15: Overview of our hyperdimensional graph representation, knowledge extraction, and transfer learning.\nevents that could indicate an emerging threat.\nBy permuting and binding hypervectors, the system can track how scenarios evolve over time, which is critical for missions that demand situational awareness across time-sensitive contexts.\n\u2022 Similarity Reasoning: HDC measures the similarity between hypervectors, allowing the system to compare different states of the knowledge graph and detect anomalies or changes in situational context.\nThis reasoning capability ensures that the AI can identify deviations from expected patterns and adjust the knowledge graph accordingly, providing real-time updates and insights to the operator.\nDynamic Knowledge Graph Evolution with Human-AI Collaboration: HDC enables the AI to dynamically update the knowledge graph based on real-time inputs from human operators.\nOperators can input new mission parameters or contextual information, which the system then integrates into the graph.\nThis collaborative interaction ensures that the knowledge graph remains current and accurately re\ufb02ects real-time situational awareness needs, facilitating seamless human-AI collaboration.\nThis is particularly relevant to the SBIR call\u2019s objective of creating user-driven, modi\ufb01able knowledge graphs that support faster, more accurate analysis and decision-making in defense operations.\nAs illustrated in Figure 14, our AI framework supports the fusion of multimodal data into the evolving knowledge graph, reasoning over it to provide transparent insights and decisions.\nThe human operator interacts with the graph to de\ufb01ne mission objectives, which the system translates into an evolving set of relationships between entities and mission-relevant data points.\nThis process allows the system to identify potential threats or anomalies based on this dynamic structure, making it highly suitable for defense operations requiring adaptive, real-time situational awareness.\nBy leveraging the dynamic knowledge graph, our framework enables high-level reasoning over complex defense mis- sions, ensuring that operators are equipped with actionable and trustworthy insights that evolve with the mission context.\nThe continuous update process, facilitated by HDC, ensures that mission objectives are consistently aligned with real-time environmental changes, providing timely decision-making support in rapidly changing operational con- ditions.\nE.1.1 Hyperdimensional Knowledge Graph: Preserving Semantic Knowledge We plan to leverage a graph-based framework to facilitate ef\ufb01cient information traversal and retrieval, catering to various applications like recommendation systems [10], question answering [7, 12], and knowledge discovery [11].\nUnlike conventional relational databases or unstructured data repositories, knowledge graphs offer a more intuitive and holistic understanding of the underlying knowledge.\nThis, in turn, streamlines the integration of diverse data sources and enables advanced reasoning and inference capabilities [15].\nIn our knowledge transfer framework, KGs are indispensable to the abstraction and incorporation of symbolic or relational knowledge.\nBy reasoning on KGs, we can decompose complex RL policies into smaller but generalizable ones, and capture the semantic information needed for computer vision tasks such as task-speci\ufb01c AI.\nFigure 15 shows an overview of our framework.\nWe exploit HDC as a transformative approach to knowledge graph memorization, exhibiting remarkable prowess in semantic preservation and cognitive guidance during transfer learn- ing.\nKnowledge graphs, intricate in their semantic linkages, are not just databases but cognitive maps that encapsulate the richness of real-world relationships and entities.\nHDC transcends traditional low-dimensional embeddings, de- ploying high-dimensional vector spaces to encode complex structures and relations.\nHDC\u2019s intrinsic capability to encode, retrieve, and manipulate high-dimensional vectors mimics the brain\u2019s own methodology, allowing for dis- tributed representations where the entirety of the graph is holistically captured.\nThis feature is not merely a technical enhancement but a cognitive leap, facilitating the encoding of context and semantics into the very fabric of KGs, thus re\ufb01ning the depth and precision of entity relationships.\nHDC\u2019s distributed vector representation ampli\ufb01es expressive- ness, enabling precise modeling and advanced reasoning.\nFurthermore, HDC excels in ef\ufb01cient similarity comparison and information retrieval, signi\ufb01cantly speeding up query processing and enhancing reasoning mechanisms.\nHDC acts as a cognitive anchor, leveraging its semantic retention to guide the application of knowledge to novel environments.\nThe robust memory graph operations, including memory reconstruction, information retrieval, and graph matching, are pivotal for learning algorithms, introducing notions of short-term/long-term memorization that 12",
        "20": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC enhance learning capabilities.\nAdditionally, HDC enables cognitive computing and reasoning over the memory graph, allowing for holographic, brain-like computation with substantial robustness against noise and failure.\nIn essence, HDC provides a robust framework where KGs can be encoded, manipulated, and retrieved with unprecedented \ufb01delity to their semantic structure, revolutionizing the way we approach knowledge transfer and the application of AI in dynamic, real-world scenarios.\nTechnically, the unique properties and functionality of HDC are particularly suitable for transferring semantic knowl- edge in the knowledge graph.\nThe high-dimensional holistic nature of the hypervectors allows adaptive control of the scope and resolution on which the knowledge graph can be operated and transferred.\nIn particular, we encode subgraphs, the size of which measures the scope, to hypervectors, the length of which measures resolution, to re\ufb02ect the attention and importance we place in the respective context and semantics they represent.\nThis feature is crucial for transfer learning not only because it is possible to retain the information of context and semantics from the knowledge graph itself, but also because it becomes possible to ignore the nuances in the environment that may not need to be transferred.\nIn particular, HDC allows adaptive control of the depth and precision of entity, relationships, and structure of the subgraphs, which, with the help of control measures such as transferability con\ufb01dence and expected structural alignment, enables adaptive and approximate subgraph matching, an algorithm essential for transferring graph.\nE.2 Statement of Work Scope: The focus of this project is to research, develop, and implement a dynamic, interactive knowledge graph framework tailored for Air Force missions requiring situational awareness, pattern-of-life analysis, and threat detec- tion.\nThe project will enhance human-AI collaboration by allowing users to interact with knowledge graphs, making modi\ufb01cations that drive further updates to the graph\u2019s structure.\nThe dynamic knowledge graph will support real-time decision-making in time-constrained environments by incorporating user input to suggest additional changes, high- light gaps, and infer new relationships.\nThe end goal is to provide an interpretable, scalable system that enables faster, more accurate situational awareness for mission-critical defense applications.\nMethodologies: We will use advanced techniques in knowledge graph construction, AI-driven reasoning, and human- machine interaction to enable ef\ufb01cient, scalable situational awareness for Air Force operations.\nKey methodologies include: \u2022 Dynamic Knowledge Graph Design: Develop and implement dynamic knowledge graphs capable of capturing en- tities, relationships, and evolving operational data.\nThe knowledge graph will be designed to handle complex mis- sions, including threat detection and targeting operations, while dynamically updating in response to user input and real-time data.\n\u2022 User-Driven Interaction: Implement a user interface that allows operators to interact with the dynamic knowledge graph, enabling modi\ufb01cations such as adding, updating, or removing entities and relationships.\nThe system will provide feedback by predicting additional necessary updates to surrounding nodes, suggesting changes to graph ontologies, and identifying inconsistencies or information gaps.\n\u2022 Semantic AI Integration: Leverage large language models (LLMs) and symbolic reasoning methods to enhance the interpretability of the knowledge graph.\nThis will allow the system to abstract complex user-de\ufb01ned missions and automatically integrate multi-modal data streams into the knowledge graph.\n\u2022 Real-Time Reasoning and Decision-Making: Use advanced AI algorithms, including neuro-symbolic reasoning and hyperdimensional computing, to analyze the knowledge graph in real time.\nThis will support decision-making by generating recommendations based on evolving mission contexts and operational data.\n\u2022 Scalability and Ef\ufb01ciency for Low-SWaP Environments: Ensure that the developed framework is optimized for resource-constrained environments (i.e., edge devices) by reducing computational load, power consumption, and reliance on large datasets.\nMilestones: The project will be structured in three phases to ensure a clear path from development to operational deployment: \u2022 Phase 1 - Conceptualization and Prototype Development (Months 1-6): Design and develop an initial prototype of the dynamic knowledge graph.\nThis phase will focus on the creation of the graph structure, user interaction mecha- nisms, and AI-driven inference models.\nCompletion of feasibility testing with preliminary performance metrics.\n\u2022 Phase 2 - Full-Scale Development and Testing (Months 7-18): Complete the full development of the dynamic knowledge graph framework, integrating user-driven interaction with real-time reasoning.\nThe system will be tested in simulated operational environments, evaluating scalability, robustness, and effectiveness.\n13",
        "21": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC \u2022 Phase 3 - Deployment and Demonstration (Months 19-24): Deploy the fully-developed system in real-world Air Force scenarios.\nThis phase will focus on on-site testing, performance validation, and optimization for edge devices.\nThe system\u2019s ability to enhance situational awareness and reduce time-to-decision in live operational settings will be evaluated.\nDeliverables: \u2022 Prototype Knowledge Graph System (Phase 1): An initial version of the dynamic knowledge graph with user-driven modi\ufb01cation features and AI reasoning capabilities.\n\u2022 Full-Scale Interactive Knowledge Graph (Phase 2): A fully-developed, real-time interactive knowledge graph inte- grated with LLM-based reasoning and multi-modal data fusion.\n\u2022 Deployment-Ready System (Phase 3): A scalable, optimized system capable of deployment in edge environments for mission-critical Air Force operations.\nIncludes the \ufb01nal report documenting all technical achievements, testing results, and user feedback.\nTasks: \u2022 Task 1: Knowledge Graph Construction and Update Mechanisms.\nDescription: Develop dynamic knowledge graph models capable of continuously updating based on user input and real-time mission data.\nCompletion: Graph models developed, capable of real-time updates in response to changing mission conditions.\n\u2013 Task 1.1: Design of Interactive Graph Schema.\nDevelop the underlying structure for the knowledge graph, including ontologies and schema for defense-speci\ufb01c entities.\nCompletion: Interactive schema and ontology structure developed.\n\u2013 Task 1.2: Real-Time Graph Update Mechanisms.\nImplement algorithms for dynamically updating graph nodes and edges as new mission data is received.\nCompletion: Graph update mechanisms implemented.\n\u2022 Task 2: User-Driven Modi\ufb01cations and AI Reasoning.\nDescription: Implement user-driven modi\ufb01cation features, allowing operators to make real-time changes to the knowledge graph, with AI assistance suggesting additional changes and highlighting inconsistencies.\nCompletion: User-driven modi\ufb01cation tools developed and integrated with the knowledge graph.\n\u2013 Task 2.1: User Interface for Knowledge Graph Interaction.\nDevelop an intuitive user interface that allows operators to interact with the knowledge graph and input mission data.\nCompletion: User interface designed and validated.\n\u2013 Task 2.2: AI-Powered Recommendations and Graph Enhancements.\nIntegrate AI reasoning to suggest updates, resolve con\ufb02icting information, and highlight gaps in the graph.\nCompletion: AI-powered recom- mendation engine implemented.\n\u2022 Task 3: Multi-Modal Data Integration and Scalability.\nDescription: Develop mechanisms for integrating multi- modal sensor data (e.g., visual, RF, LiDAR) into the dynamic knowledge graph and optimize the system for operation on low-SWaP devices.\nCompletion: Multi-modal data fusion algorithms developed, and system optimized for edge environments.\n\u2013 Task 3.1: Sensor Data Fusion in Knowledge Graph.\nImplement methods for fusing and interpreting multi- modal data streams within the graph.\nCompletion: Sensor data fusion successfully integrated.\n\u2013 Task 3.2: Optimization for Edge Devices.\nReduce the computational load and energy consumption to ensure the system runs ef\ufb01ciently on low-SWaP platforms.\nCompletion: System optimization for edge environments achieved.\nTask Deliverable Timeline and Deliverables: All deliv- erables and completion times are listed in Table 1.\nPhase II will consist of four main tasks focusing on the development and implementation of a robust and dy- namic knowledge graph for the predic- tion of complex missions and providing situational awarness.\nThis phase will integrate these models for robust real- world applications, extending to complex and noisy data environments, and will culminate in the deployment of a Knowledge graph schema and ontology Q1 2025 Q2 2025 Real-time graph update algorithms Q2 2025 User interface for graph interaction Q3 2025 AI recommendation engine Q3 2025 Multi-modal data fusion algorithms Q4 2025 Edge device optimization Q4 2025 Fully-developed and deployed system Task 1.1 Task 1.2 Task 2.1 Task 2.2 Task 3.1 Task 3.2 Final Deliverable Table 1 Deliverables by Task and Completion.\nCompletion 14",
        "22": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC comprehensive knowledge transfer system, which will be tested on both simulations and physical platforms.\nMultiple demos and team participation in related competitions will feature throughout the project.\nThe following table outlines a two-phase, 6-month project with a $250,000 total budget aimed at accomplishing all tasks.\nAdditional \ufb01nancial details are provided in the budget justi\ufb01cation.\nF Metrics for Success in Phase I and Phase II F.1 Phase I Metrics Phase I of the project will focus on demonstrating the feasibility of the dynamic knowledge graph system for en- hancing situational awareness, pattern-of-life analysis, and threat detection in time-constrained environments.\nThe primary success metrics will concentrate on accuracy, graph completeness, processing speed, and resource ef\ufb01ciency.\nThese metrics will help in comparing the system\u2019s performance with baseline models to ensure its compliance with operational needs outlined in the SBIR call.\nAccuracy Accuracy is crucial to ensuring that the dynamic knowledge graph represents and reasons over mission- speci\ufb01c data effectively.\nWe will evaluate two major aspects of accuracy: the ability of the system to correctly classify entities (nodes) and relationships (edges) and its capacity to predict the likelihood of events based on historical mission data stored within the graph.\nFor Phase I, the target is to achieve over 85% mean Average Precision (mAP) at an Intersection over Union (IoU) threshold of 0.90.\nIn addition, the system should deliver more than 90% classi\ufb01cation accuracy for dynamic knowledge graph updates, ensuring reliable identi\ufb01cation of threats and events.\nGraph Completeness Graph completeness measures how effectively the system integrates various data sources and \ufb01lls information gaps as mission conditions evolve.\nThis will be assessed by monitoring the average number of connections (edges) each node maintains and evaluating the system\u2019s ability to dynamically update these connections.\nAnother important measure is the system\u2019s ability to cover the critical ontologies required for Air Force missions.\nThe target for Phase I is to achieve more than 95% graph completeness in operational contexts, with 90% coverage of mission-speci\ufb01c ontologies, ensuring that all critical entities and relationships are accurately represented.\nProcessing Speed In time-sensitive environments, the system\u2019s processing speed will be a critical factor for success.\nWe will measure the system\u2019s response time when new information is introduced or when users modify the knowledge graph.\nThe two main metrics are the graph update latency and the time taken to infer new edges or classify new nodes.\nThe target for Phase I is to maintain an inference time of less than 25 milliseconds per node or edge and ensure that graph updates occur within 100 milliseconds.\nThese processing speeds are necessary to support real-time mission operations effectively.\nResource Ef\ufb01ciency (Low-SWaP Optimization) The dynamic knowledge graph system will need to operate ef\ufb01- ciently on low-Size, Weight, and Power (low-SWaP) devices typical of defense systems.\nThe primary resource ef\ufb01- ciency metrics will be memory usage and power consumption.\nThe system should utilize less than 80MB of memory and operate with less than 5 watts of power, ensuring that it is compatible with the constraints of edge devices typically used in Air Force missions.\n>85% at IoU 0.90 Graph Completeness Target Value >90% accuracy Metric Node and Edge Classi\ufb01cation Ac- curacy Mean Average Precision (mAP) F.2 Phase II: Multimodal Information Processing Using Dynamic Knowledge Graphs In Phase II, we aim to leverage the dynamic knowledge graph (KG) framework, generated and maintained by large language models (LLMs), to enable robust multimodal information process- ing.\nThe system will support data inputs from various modal- radar, including camera, ities, and RF sensors, to ensure com- prehensive situational awareness in real-time operational environ- ments.\nBy incorporating these diverse data streams, the knowledge graph will provide a uni\ufb01ed, interpretable, and actionable framework for real- time decision-making, threat detection, and pattern-of-life analysis.\nThe LLM-driven KG will integrate multimodal data into a transparent structure, ensuring traceability of decisions and Measurement Method Benchmark tests with mission- speci\ufb01c data Performance Force datasets Graph audits and completeness as- sessments Automated ontology validation Real-time system performance tests Timed tests on update operations Monitoring system resource utiliza- tion Testing on low-SWaP hardware Ontology Coverage Inference Time per Node/Edge Graph Update Latency Memory Usage Table 2 Quantitative metrics for dynamic knowledge graph success.\n90% coverage <25ms per input <100ms <80MB >95% completeness evaluation on Air Power Consumption <5W 15",
        "23": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC recommendations.\nThis transparency is essential for human operators to understand the system\u2019s reasoning process, particularly in tasks like threat assessment, post-engagement evaluation, and decision-making in high-pressure envi- ronments.\nFor instance, sensor data from cameras and radar can be correlated with RF signals to detect anomalies or suspicious activity, which the knowledge graph will represent in real-time, supporting effective human-AI collabora- tion.\nIn addition to multimodal integration, we plan to enable several advanced tasks as part of Phase II, including those highlighted in the SBIR call.\nThese tasks include: \u2022 Threat Assessment: Identifying potential threats by combining multimodal data streams (e.g., camera, radar, RF) in real time.\n\u2022 Post-Engagement Analysis: Assessing the operational status of assets post-engagement by dynamically updating the knowledge graph with new sensor data.\n\u2022 Decision-Making: Providing recommendations for resource allocation, mission updates, or defensive measures based on real-time changes in the operational environment.\n\u2022 Situational Awareness and Pattern-of-Life Analysis: Using the knowledge graph to continuously update and provide context-aware situational awareness based on evolving mission conditions.\nBy integrating multimodal data in a dy- namic knowledge graph, we ensure that op- erators have a clear, interpretable, and real- time overview of complex missions, en- abling faster and more accurate decisions.\nThe system will support user-driven modi\ufb01- cations, allowing for interactive updates that re\ufb01ne the graph structure based on new in- formation or changing mission objectives.\nMetric Mean Average Precision (mAP) for Threat Detection Graph Update Latency for Multimodal In- puts (Camera, Radar, RF) Classi\ufb01cation Engagement Analysis Inference Time for Decision-Making Multimodal Data Fusion (Completeness) Accuracy Post- for Target Value >85% at IoU 0.90 <100ms >90% <25ms per input >95% for integrated modalities Table 3 Quantitative metrics for multimodal information processing in Phase II, supporting camera, radar, and RF sensors.\nF.3 Team Quali\ufb01cations.\nF.3.1 Key Personnel AI-Sensation LLC, led by Dr.\nMohsen Imani, President and a faculty member at the University of California Irvine, is at the forefront of arti\ufb01cial cognitive intelligence.\nOur startup\u2019s core expertise lies in neuro-symbolic AI, a pioneering \ufb01eld where Dr.\nImani is a globally recognized authority.\nAI-Sensation has been actively engaged in multiple successful technology transfers to the Department of Defense (DoD), solidifying its reputation through contracts with DARPA and the Air Force.\nThis proven track record underscores our capability to handle complex defense-related AI projects, particularly in enhancing object detection systems within the DoD.\nOur team has also been collaborating closely with multiple tech companies, including CISCO, Intel, and IBM, where multiple technologies have been considered for commercialization as a part of this collaboration.\nThis project is going to have the following key people with high quali\ufb01cations and several related experiences.\n\u2022 Dr.\nMohsen Imani (Principal Investigator): Employer: AI Sensation LLC , Foreign National: No Quali\ufb01cations: Dr.\nImani received his Ph.D.\nin Computer Science from the Department of Computer Science and Engineering, University of California San Diego (UCSD) in 2020.\nDr.\nImani is one of the pioneers in the area of hyperdimensional reasoning and its applications in the cognitive learning domain.\nHis contributions have paved a new path in brain-inspired HDC, enabling ultra-ef\ufb01cient and real-time cognitive learning.\nHis research has been a key factor in initiating multiple programs at the Semiconductor Research Corporation (SRC), the Defense Advanced Research Projects Agency (DARPA), Intel, IBM, and CISCO.\nDr.\nImani\u2019s research has been a rising start in defense industry, has earned several prestigious awards, including the DARPA Young Faculty Award, the SRC Young Fac- ulty Award, the ONR Young Investigator Program Award, Army Early Career Award, the DARPA Riser Award, the Bernard Gordon Engineering Leadership Award, and the UCSD Outstanding Researcher Award.\nHe has also received seven best paper awards and nominations at top conferences.\nDr.\nImani has a long history of successful technology transfers to multiple companies (e.g., Intel, Qualcomm, IBM, and Cisco) and governmental agencies (e.g., DARPA, ONR, and Air Force).\nAI-Sensation thrives on innovation facilitated by the Bio-Inspired Architecture and System Laboratory (BiasLab) at UC Irvine, where Dr.\nImani leads a team of over 50 members, including 24 Ph.D.\nstudents and 4 postdocs.\nThis af\ufb01liation allows AI-Sensation unrestricted access to cutting-edge resources and research facilities, amplifying our capability to deliver advanced AI solutions.\nPublications: Dr.\nImani boasts an impressive publication record with over 250 papers in top conferences and journals and holds 20 U.S.\npatents.\nRelated to this project, we can highlight his papers that were accepted and presented/published in the prestigious Intern.\n16",
        "24": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC \u2022 Dr.\nBehnam Khaleghi (Research Scientist): Employer: AI Sensation LLC , Foreign National: No Quali\ufb01cations: Dr.\nBehnam Khaleghi is a highly accomplished researcher and engineer at Qualcomm, specializing in the development of advanced XR systems.\nHe earned his PhD in Computer Science from the University of California, San Diego, where he was recognized for his outstanding research with multiple honors, including being a \ufb01nalist for the Qualcomm Innovation Fellowship in 2022.\nDr.\nKhaleghi\u2019s expertise lies in the \ufb01eld of computer vision and AI/ML, particularly in developing robust object detection systems.\nHis work involves creating hybrid AI/ML models that integrate traditional computer vision techniques, such as scale-invariant feature transform and edge detection, with modern approaches like neural networks and evolutionary algorithms.\nThis combination allows for ef\ufb01cient processing and analysis.\nIn addition to his technical skills, Dr.\nKhaleghi has a strong track record of innovation and collaboration, as evidenced by his multiple best-paper nominations at international conferences and his ability to secure prestigious research fellowships.\nHis contributions to the \ufb01eld of computer vision are well- regarded both in academic circles and in the industry, making him a valuable asset to any project aimed at advancing AI/ML technologies for object detection and scene analysis.\nPublications: Dr.\nKhaleghi has authored/co-authored more than 66 peer-reviewed papers in top conferences/jour- nals.\nHe has been granted eight U.S.\nPatents with more than 7 non-provisional patents pending.\nIn collaboration with Intel, IBM, and Qualcomm, Dr.\nKhaleghi addressed the long-time inaccuracy and data dependency of the ob- ject detection model by developing a hybrid architecture that leverages the best of both worlds: deep learning and symbolic AI.\nHe has also expertise in multiple chip design and fabrication.\nF.3.2 Comparison with Existing Solutions Our dynamic knowledge graph framework, driven by large language models and hyperdimensional cognitive learn- ing, signi\ufb01cantly outperforms existing solutions in several key areas: accuracy, automation, and processing speed.\nTraditional AI-based systems often rely on static models and prede\ufb01ned relationships, which limits their adaptabil- ity in dynamic and time-constrained environments.\nThese systems frequently require manual intervention to update graphs and manage new data inputs, leading to delays and inef\ufb01ciencies in high-stakes defense operations.\nIn contrast, our dynamic knowledge graph approach enables real-time, automated updates based on user input and sensor data, drastically reducing the need for manual adjustments and improving decision-making speed.\nExisting systems also struggle with maintaining high accuracy when processing multimodal data from diverse sources such as cameras, radar, and RF sensors.\nThese approaches typically require extensive preprocessing or manual curation of data, which can compromise real-time situational awareness.\nOur LLM-driven framework, however, is designed to seamlessly integrate and fuse multimodal data into a uni\ufb01ed knowledge graph, leveraging symbolic reasoning and neuro-symbolic AI to deliver transparent, high-con\ufb01dence decisions.\nThis results in higher accuracy for tasks such as threat detection and post-engagement analysis, as our system can dynamically adapt to new data inputs without compromising performance.\nIn particular, the system achieves more than 85% mean Average Precision (mAP) for threat detection with less than 100ms latency for graph updates\u2014a substantial improvement over existing manual or semi-automated approaches.\nFurthermore, existing knowledge graph systems often lack transparency and interpretability, functioning as black-box models where the reasoning behind decisions is not easily understood by human operators.\nOur approach leverages symbolic reasoning and semantic AI to ensure that every decision or update made to the knowledge graph is fully traceable and interpretable.\nThis transparency is crucial in defense applications, where human operators must trust the system\u2019s output in critical situations.\nBy combining reasoning over multimodal data streams with high interpretability, our system enables more accurate and trustworthy decision-making compared to conventional approaches.\nF.3.3 Past Work Multiple past works are listed in the feasibility documents.\nHere, we brie\ufb02y list our related projects: Reasoning-Guided Prediction: Our team has extensive experience in reasoning-based learning and prediction.\nOur hybrid AI approach learns from limited samples, embedding information via deep learning into space learnable by a symbolic AI model.\nThe symbolic AI reasons over this information to make decisions about the object of interest [3, 6, 13].\nThis includes analogical reasoning over semantic graphs to identify relevant activities.\nOur neuro-symbolic approach, central to this SBIR project, leverages the vision-language model to generate items and features for user- de\ufb01ned tasks, reducing data requirements to fewer than 20 samples for complex tasks, thus revolutionizing defense applications requiring robust, transparent detection.\n17",
        "25": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC Hyperdimensional Cognitive Learning: We developed novel object detection models resilient to extreme noise in hardware and input data.\nThese algorithms operate in high-dimensional space, mapping data into holographic rep- resentations for better separation and representation [8, 16].\nThis learning approach is both ef\ufb01cient and robust, tolerating up to 30% noise while maintaining quality [2], making it ideal for defense applications where data or hard- ware may be compromised.\nRecent work focuses on reliable learning on unreliable platforms and its potential in various scenarios [1].\nWe also advance knowledge graph representation and reasoning through scalable methods that overcome interpretability and ef\ufb01ciency limitations.\nOur Conjunctive Block Coding for Hyperdimensional Graph Representation (CLOG) leverages Hyperdimensional Computing to preserve structural similarities and improve rea- soning [8, 14].\nThese innovations are critical for robust AI in real-time decision-making.\nIntegrating semantic AI, our framework generalizes complex, user-de\ufb01ned missions, ensuring interpretability and ef\ufb01ciency in processing high- throughput multimodal data.\nQuantum computing offers promising scalability, enhancing semantic AI ef\ufb01ciency in real-time mission detection [9].\nF.3.4 Risks & Mitigation Plans While we assess the overall risk of this project as low, based on our extensive experience with DoD and industry partners, as well as our proven technologies, we recognize potential risks that could impact the successful completion of this project.\nTo ensure project success, we have identi\ufb01ed these risks and implemented robust mitigation strategies to address them at every stage.\n(1) Technical Risks: Developing a dynamic knowledge graph framework that integrates real-time, context-aware processing of high-throughput multimodal data (e.g., camera, radar, RF sensors) is inherently complex.\nThe risk lies in ensuring that the system is able to handle large volumes of diverse data streams while maintaining high accuracy and ef\ufb01ciency.\nTo mitigate this, we will employ a modular development approach, allowing for iterative testing and integration of individual components.\nBy isolating system modules (such as the LLM-based knowledge graph generation, multimodal data fusion, and real-time reasoning engine), we can test and optimize each one independently before full integration.\nTransfer learning and data augmentation techniques will further enhance the robustness and generalization of our models, even with limited or noisy data inputs.\nAdditionally, we will conduct extensive validation across diverse datasets and operational environments to ensure system reliability and scalability.\n(2) Data-Related Risks: One of the key challenges in this project involves ensuring the quality and completeness of the multimodal data (e.g., sensor data, real-time mission information).\nPoor or incomplete data can signi\ufb01cantly impact the system\u2019s ability to generate accurate and actionable knowledge graphs.\nTo address this, we have devised a comprehensive data collection plan that combines both synthetic and real-world datasets.\nCollaboration with industry and access to publicly available datasets will help ensure the diversity and quality of data inputs.\nMoreover, advanced sensor technologies and semi-automated labeling techniques will be used to ensure accurate and ef\ufb01cient data capture, minimizing manual efforts and reducing the risk of erroneous data affecting system performance.\n(3) Integration and Operational Risks: The complexity of integrating various hardware platforms (edge devices, sensors, and AI processors) and ensuring smooth operation in real-time mission-critical environments poses an oper- ational risk.\nTo mitigate this, we will maintain a detailed project plan with clearly de\ufb01ned milestones, deliverables, and contingency buffers.\nRegular progress reviews and agile development practices will allow us to quickly identify potential issues and adjust resource allocation as needed.\nAdditionally, by simulating operational scenarios early in the development phase, we will ensure that the system is capable of scaling ef\ufb01ciently in real-world applications.\nA continuous feedback loop with end-users (e.g., defense operators) will also be implemented to ensure the system meets operational requirements at all stages.\n(4) Environmental Risks: The performance of our system across varied environmental conditions is a critical factor, especially in defense applications where conditions can range from extreme temperatures to high-noise environments.\nTo mitigate this, we will design our AI models to be adaptable and robust in diverse environments by implementing adaptive algorithms capable of self-tuning based on real-time sensor feedback.\nAdditionally, models will be optimized for operation on low-SWaP (Size, Weight, and Power) edge devices through hardware acceleration and lightweight computation techniques.\nExtensive \ufb01eld testing in varied operational environments will be conducted to ensure the system remains reliable and responsive under changing conditions.\nBy addressing these risks proactively, with strategies grounded in extensive experience and initial successes, we are con\ufb01dent in our ability to meet project objectives and deliver a robust, scalable, and ef\ufb01cient system.\nOur modular approach, comprehensive validation strategy, and strong emphasis on real-time adaptability and resource optimization will ensure that the project remains on track and within scope.\n18",
        "26": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC G Commercialization Plan for Dynamic Knowledge Graph Framework The commercialization strategy for the Dynamic Knowledge Graph (DKG) framework builds upon AI-Sensation\u2019s established relationships with key defense and industrial partners.\nThis strategy focuses on transitioning the technology from research and development (R&D) into real-world applications that address critical needs across military and commercial sectors.\nWith Phase III speci\ufb01cally targeting both military and commercial markets, the DKG framework is designed to provide scalable, versatile solutions beyond situational awareness, with applications in sectors such as defense, homeland security, \ufb01nancial services, and industrial monitoring.\nOur deep engagement with Air Force stakeholders and multiple branches of the U.S.\nmilitary, combined with strong partnerships with industry leaders, ensures that this technology will meet operational and commercial requirements.\nMilitary and Homeland Security Applications: AI-Sensation is actively collaborating with the Air Force Research Laboratory (AFRL) Information Science Directorate on multiple projects, laying a strong foundation for the integra- tion of the DKG framework into critical Air Force operations.\nThese collaborations focus on advancing real-time situational awareness systems that enhance decision-making in highly dynamic and time-sensitive environments, di- rectly addressing the operational challenges faced by the Air Force.\nAdditionally, AI-Sensation holds several active contracts with the U.S.\nNavy, which involve improving battle damage assessment and threat detection systems.\nThis includes engagements with the Naval Surface Warfare Center Dahlgren Division and the U.S.\nNaval Undersea War- fare Center.\nThese existing contracts showcase the proven relevance of our technology for improving intelligence operations and threat detection across multiple military branches.\nThe DKG framework, with its dynamic adaptability and real-time processing capabilities, offers signi\ufb01cant advantages for military applications.\nIt provides mission-critical support for pattern-of-life analysis, predictive threat detection, and automated targeting operations.\nThis technology\u2019s potential to streamline and automate complex analysis tasks makes it an invaluable tool for Air Force intelligence analysts and operational planners.\nMoreover, the system\u2019s ability to continuously update based on changing conditions and user inputs ensures that it remains responsive to rapidly evolving mission parameters.\nBeyond Air Force operations, the DKG framework can also be applied to homeland security efforts, providing critical infrastructure monitoring and early identi\ufb01cation of emerging risks, including cy- berattacks, terrorism, and natural disasters.\nThrough collaboration with homeland security agencies, AI-Sensation plans to tailor the DKG system for national security and emergency response applications, ensuring the protection of vital assets and the public.\nCommercial Applications: The commercial potential of the DKG framework is equally compelling, with a wide range of industries set to bene\ufb01t from its real-time data processing capabilities.\nOne prominent application is \ufb01nan- cial fraud detection, where the DKG framework\u2019s ability to process and analyze vast amounts of multimodal data in real-time can signi\ufb01cantly enhance the detection of sophisticated fraud schemes.\nIts advanced pattern recognition and anomaly detection capabilities allow businesses to identify irregularities and potential threats much faster than traditional methods.\nAdditionally, in industrial monitoring, the DKG framework can be used to track equipment per- formance, predict failures, and monitor supply chains, providing early warnings that minimize downtime and reduce \ufb01nancial losses.\nAI-Sensation\u2019s strategic partnerships with major industry players such as Intel, IBM, Qualcomm, and Cisco\u2014supported through direct industrial contracts and the CHIPS Act\u2014demonstrate our commitment to translating this technology into actionable solutions for commercial markets.\nThese collaborations enable us to leverage the latest advancements in hardware and software, ensuring that the DKG framework remains at the forefront of technological innovation.\nOur partnerships allow us to integrate the DKG framework into diverse commercial sectors, from \ufb01nance to manufactur- ing, telecommunications, and beyond.\nThe ability of the DKG system to scale and adapt to different industry-speci\ufb01c needs makes it a versatile tool with broad market applicability.\nPhase III Commercialization Strategy: The commercialization strategy for the DKG framework follows a well- structured and phased approach, ensuring the successful deployment of the technology across military and commercial sectors: \u2022 Military Integration (Air Force/DoD) \u2013 Following the completion of Phase II, AI-Sensation will collaborate closely with Air Force stakeholders to conduct rigorous testing and re\ufb01nement of the DKG framework.\nOur ongoing projects with AFRL will enable us to tailor the DKG\u2019s capabilities to meet speci\ufb01c Air Force mission pro\ufb01les, ensuring it can handle the operational demands of real-time military decision-making.\nIn parallel, our existing contracts with the U.S.\nNavy, particularly in the areas of battle damage assessment and threat detection, offer a clear pathway for integrating the DKG framework into Navy intelligence systems.\nThese contracts, which include work with the Naval Surface Warfare Center Dahlgren Division and the U.S.\nNaval Undersea Warfare Center, provide a proven use case for the DKG\u2019s applications in multi-domain military operations.\n19",
        "27": "Proposal Number: F244-0001-0057 Topic Number: AF244-0001 Firm Name: AI Sensation LLC \u2022 Homeland Security \u2013 Beyond military applications, the DKG framework is highly relevant for homeland security, where it can be used to monitor critical infrastructure, detect cybersecurity threats, and provide early warnings of potential risks such as terrorism and natural disasters.\nAI-Sensation will work with homeland security agencies to customize the system for real-time monitoring and response, ensuring that it meets the operational needs of agencies responsible for national defense and emergency preparedness.\nThis collaboration will help bring the DKG system into widespread use in homeland security applications.\n\u2022 Commercial Markets \u2013 The DKG framework has signi\ufb01cant potential for deployment in commercial sectors where real-time data processing is critical.\nOur established partnerships with industry leaders through direct contracts and the CHIPS Act\u2014speci\ufb01cally with Intel, IBM, Qualcomm, and Cisco\u2014will facilitate the scaling of the DKG frame- work across multiple commercial applications.\nThese collaborations will allow us to market the DKG framework to sectors such as \ufb01nance, manufacturing, telecommunications, and cybersecurity.\nThe versatility of the framework ensures that it can be customized to address the speci\ufb01c data processing and predictive analytics needs of different industries, making it a valuable tool for improving operational ef\ufb01ciency and risk management.\nThis approach ensures that the DKG system will be a versatile and powerful tool, with applications across defense, homeland security, and commercial industries.\nOur strong relationships with military stakeholders, including the Air Force, Navy, and Army, along with our industrial partnerships with leading technology companies, provide a robust foundation for successful commercialization.\nThe DKG framework\u2019s real-time adaptability, transparency, and ef\ufb01ciency make it an essential asset for both military and commercial applications, positioning it for broad market success.\nReferences [1] Hamza Barkam, Sanggeon Yun, Hanning Chen, Paul Gensler, Albi Mema, Andrew Ding, Hussam Amrouch, and Mohsen Imani.\nReliable hyperdimensional reasoning on unreliable emerging technologies.\nIn ICCAD.\nIEEE, 2023.\n[2] Alejandro Cano, Nami Matsumoto, Eric Ping, and Mohsen Imani.\nOnlinehd: Robust, ef\ufb01cient, and single-pass online learning using hyperdimensional system.\nIn DATE.\nIEEE, 2021.\n[3] Hanning Chen, Wenjun Huang, Yang Ni, Sanggeon Yun, Fei Wen, Hugo Latapie, and Mohsen Imani.\nTaskclip: Extend large vision-language model for task oriented object detection.\narXiv preprint arXiv:2403.08108, 2024.\n[4] Hanning Chen, Yang Ni, Ali Zakeri, Zhuowen Zou, Sanggeon Yun, Fei Wen, Behnam Khaleghi, Narayan Srinivasa, Hugo Latapie, and Mohsen Imani.\nHdreason: Algorithm-hardware codesign for hyperdimensional knowledge graph reasoning.\narXiv preprint arXiv:2403.05763, 2024.\n[5] Graph RAG.\n.\nhttps://microsoft.github.io/graphrag/, FreePDK 45nm Library.\n[6] Wenjun Huang, Arghavan Rezvani, Hanning Chen, Yang Ni, Sanggeon Yun, Sungheon Jeong, and Mohsen Imani.\nA plug-in tiny ai module for intelligent and selective sensor data transmission.\narXiv preprint arXiv:2402.02043, 2024.\n[7] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li.\nKnowledge graph embedding based question answering.\nIn Proceedings of the twelfth ACM international conference on web search and data mining, pages 105\u2013113, 2019.\n[8] Prathyush Poduval, Ali Zakeri, Farhad Imani, Hassan Naja\ufb01, Tony Givargis, and Mohsen Imani.\nGraphd: Graph-based hyperdimensional memorization for brain-like cognitive learning.\nFrontiers in Neuroscience, 16:5, 2022.\n[9] Prathyush Prasanth Poduval, Zhuowen Zou, and Mohsen Imani.\nHdqmf: Holographic feature decomposition using quantum algorithms.\nIn CVPR, pages 10978\u201310987, 2024.\n[10] Bilin Shao, Xiaojun Li, and Genqing Bian.\nA survey of research hotspots and frontier trends of recommendation systems from the perspective of knowledge graph.\nExpert Systems with Applications, 165:113764, 2021.\n[11] Xiaohui Tao, Thuan Pham, Ji Zhang, Jianming Yong, Wee Pheng Goh, Wenping Zhang, and Yi Cai.\nMining health knowledge graph for health risk prediction.\nWorld Wide Web, 23:2341\u20132362, 2020.\n[12] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec.\nQa-gnn: Reasoning with language models and knowledge graphs for question answering.\narXiv preprint arXiv:2104.06378, 2021.\n[13] Sanggeon Yun, Ryozo Masukawa, Minhyoung Na, and Mohsen Imani.\nMissiongnn: Hierarchical multimodal gnn- based weakly supervised video anomaly recognition with mission-speci\ufb01c knowledge graph generation.\narXiv preprint arXiv:2406.18815, 2024.\n[14] Ali Zakeri, Zhuowen Zou, Hanning Chen, Hugo Latapie, and Mohsen Imani.\nConjunctive block coding for hyperdimensional graph representation.\nIntelligent Systems with Applications, 22:200353, 2024.\n[15] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun.\nGraph neural networks: A review of methods and applications.\nAI open, 1:57\u201381, 2020.\n[16] Zhuowen Zou, Yeseong Kim, Farhad Imani, Haleh Alimohamadi, Rosario Cammarota, and Mohsen Imani.\nScalable edge- based hyperdimensional learning system with brain-like neural adaptation.\nIn SC, pages 1\u201315, 2021.\n20",
        "28": "SBIR Phase I Proposal Proposal Number Topic Number Proposal Title Date Submitted Firm Information Firm Name Mail Address Website Address UEI Cage F244-0001-0057 AF244-0001 AI-Generated Dynamic Knowledge Graph for Situational Awareness 10/19/2024 11:33:33 PM AI Sensation 26766 Ashford, MISSION VIEJO, California, 92692 http://www.ai-sensation.com/ W2XXHSDNL8A4 9RE77 Total Dollar Amount for this Proposal Base Year Year 2 Technical and Business Assistance(TABA)- Base TABA- Year 2 Base Year Summary Total Direct Labor (TDL) Total Direct Material Costs (TDM) Total Direct Supplies Costs (TDS) Total Direct Equipment Costs (TDE) Total Direct Travel Costs (TDT) Total Other Direct Costs (TODC) G&A (rate 22%) x Base () Total Firm Costs Subcontractor Costs Total Subcontractor Costs (TSC) Cost Sharing Profit Rate (7%) Total Estimated Cost TABA Year 2 Summary Total Direct Labor (TDL) Total Direct Material Costs (TDM) Total Direct Supplies Costs (TDS) $139,997.09 $69,890.69 $70,106.40 $0.00 $0.00 $65,318.40 $0.00 $0.00 $0.00 $0.00 $0.00 $0.00 $65,318.40 $0.00 -$0.00 $4,572.29 $69,890.69 $0.00 $65,520.00 $0.00 $0.00",
        "29": "Total Direct Equipment Costs (TDE) Total Direct Travel Costs (TDT) Total Other Direct Costs (TODC) G&A (rate 22%) x Base () Total Firm Costs Subcontractor Costs Total Subcontractor Costs (TSC) Cost Sharing Profit Rate (7%) Total Estimated Cost TABA Base Year Direct Labor Costs Category / Individual-TR Computer and Information Research Scientist/ Principal Investigator (Mohsen Imani) Subtotal Direct Labor (DL) Labor Overhead (rate 20%) x (DL) Total Direct Labor (TDL) G&A (rate 22%) x Base () Cost Sharing Profit Rate (7%) Total Estimated Cost TABA Year 2 Direct Labor Costs Category / Individual-TR Computer and Information Research Scientist/ Principal Investigator (Mohsen Imani) Subtotal Direct Labor (DL) Labor Overhead (rate 20%) x (DL) Total Direct Labor (TDL) $0.00 $0.00 $0.00 $0.00 $65,520.00 $0.00 -$0.00 $4,586.40 $70,106.40 $0.00 Rate/Hour Estimated Hours Fringe Rate (%) Fringe Cost Cost $140.00 324 20 $9072.00 $54,432.00 $54,432.00 $10,886.40 $65,318.40 $0.00 -$0.00 $4,572.29 $69,890.69 $0.00 Rate/Hour Estimated Hours Fringe Rate (%) Fringe Cost Cost $140.00 325 20 $9100.00 $54,600.00 $54,600.00 $10,920.00 $65,520.00",
        "30": "G&A (rate 22%) x Base () Cost Sharing Profit Rate (7%) Total Estimated Cost TABA Explanatory Material Relating to the Cost Volume The Official From the Firm that is responsible for the cost breakdown Name: Mohsen Imani Phone: (619) 549-9084 Phone: m.imani@ai-sensation.com Title: Proposal Owner $0.00 -$0.00 $4,586.40 $70,106.40 $0.00 If the Defence Contracting Audit Agency has performed a review of your projects within the past 12 months, please provide: No Select the Type of Payment Desired: Partial payments",
        "31": "Cost Volume Details Direct Labor Base Category Description Education Yrs Experience Hours Rate Fringe Rate Total Computer and Information Research Scientist Principal Investigator PhD 8 324 $140.00 20 $54,432.00 Are the labor rates detailed below fully loaded?\nYES Please explain any costs that apply.\nBudget Justification Attached.\nProvide any additional information and cost support data related to the nature of the direct labor detailed above.\nBudget Justification Attached.\nLabor rate Documentation: \u2022 Budget Justification.pdf Direct Labor Cost ($): Year2 $54,432.00 Category Description Education Yrs Experience Hours Rate Fringe Rate Total Computer and Information Research Scientist Principal Investigator PhD 8 325 $140.00 20 $54,600.00 Are the labor rates detailed below fully loaded?\nYES Please explain any costs that apply.\nBudget Justification Attached.\nProvide any additional information and cost support data related to the nature of the direct labor detailed above.\nBudget Justification Attached.",
        "32": "Direct Labor Cost ($): Sum of all Direct Labor Costs is($): Overhead Base Labor Cost Overhead Rate (%) Overhead Comments: Overhead Cost ($): Year2 Labor Cost Overhead Rate (%) Overhead Comments: Overhead Cost ($): Sum of all Overhead Costs is ($): General and Administration Cost Base G&A Rate (%): Apply G&A Rate to Overhead Costs?\nApply G&A Rate to Direct Labor Costs?\nPlease specify the different cost sources below from which your company's General and Administrative costs are calculated.\nG&A Cost ($): Year2 G&A Rate (%): Apply G&A Rate to Overhead Costs?\nApply G&A Rate to Direct Labor Costs?\n$54,600.00 $109,032.00 20 $10,886.40 20 $10,920.00 $21,806.40 22 NO NO $0.00 22 NO NO",
        "33": "Please specify the different cost sources below from which your company's General and Administrative costs are calculated.\nG&A Cost ($): Sum of all G&A Costs is ($): Profit Rate/Cost Sharing Base Cost Sharing ($): Cost Sharing Explanation: Profit Rate (%): Profit Explanation: Total Profit Cost ($): Year2 Cost Sharing ($): Cost Sharing Explanation: Profit Rate (%): Profit Explanation: Total Profit Cost ($): Total Proposed Amount ($): $0.00 $0.00 -$0.00 7 $9,158.69 -$0.00 7 $9,158.69 $139,997.09",
        "34": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd",
        "35": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd",
        "36": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd",
        "37": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd",
        "38": "\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd",
        "39": "Mohsen Imani, AI Sensation Oct 19, 2024 Oct 19, 2025"
    },
    "images": [
        {
            "page": 8,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page8_img1.jpeg",
            "hash": "96d9cd96694d920d",
            "position": "Bottom Right"
        },
        {
            "page": 8,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page8_img2.jpeg",
            "hash": "f3b04cd5e23998cc",
            "position": "Bottom Center"
        },
        {
            "page": 8,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page8_img3.jpeg",
            "hash": "999967646626339b",
            "position": "Bottom Right"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img1.png",
            "hash": "81550957255e35fb",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img6.png",
            "hash": "bb2e4b0bb439c4d1",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img7.png",
            "hash": "b19b31e64c646f31",
            "position": "Middle Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img10.jpeg",
            "hash": "be2dc1da97936830",
            "position": "Bottom Right"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img11.jpeg",
            "hash": "9052856a5f695e6b",
            "position": "Bottom Right"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img13.jpeg",
            "hash": "849a4b277cce356c",
            "position": "Bottom Right"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img14.png",
            "hash": "c29a3765e961e68c",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img15.jpeg",
            "hash": "905285615f695e6f",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img19.png",
            "hash": "cb982c58fc238cdd",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img20.jpeg",
            "hash": "95b13a52727963c6",
            "position": "Middle Right"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img21.jpeg",
            "hash": "b8c29758c39dc437",
            "position": "Middle Right"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img22.jpeg",
            "hash": "95b13a52727963c6",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img23.png",
            "hash": "87ce69653c6a3293",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img27.jpeg",
            "hash": "dd42829deb102d6f",
            "position": "Bottom Right"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img28.png",
            "hash": "bb98b072a58dc7e0",
            "position": "Bottom Center"
        },
        {
            "page": 9,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page9_img31.jpeg",
            "hash": "811e7821a79c7b67",
            "position": "Bottom Center"
        },
        {
            "page": 10,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page10_img1.png",
            "hash": "80802a2a2a2a8080",
            "position": "Middle Right"
        },
        {
            "page": 10,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page10_img2.png",
            "hash": "80802a2a2a2a8080",
            "position": "Middle Right"
        },
        {
            "page": 10,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page10_img3.png",
            "hash": "ea789585c39e9e88",
            "position": "Middle Center"
        },
        {
            "page": 10,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page10_img4.png",
            "hash": "89d35f4f4e2ca438",
            "position": "Middle Center"
        },
        {
            "page": 10,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page10_img5.jpeg",
            "hash": "c6b461ce12e70d9b",
            "position": "Middle Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img1.jpeg",
            "hash": "d906e6aeb8d96126",
            "position": "Top Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img2.jpeg",
            "hash": "b995524e64e16b5c",
            "position": "Middle Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img3.jpeg",
            "hash": "ea95a5c491d19567",
            "position": "Top Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img8.png",
            "hash": "bc3cd39be072c04d",
            "position": "Middle Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img11.png",
            "hash": "81550957255e35fb",
            "position": "Top Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img12.png",
            "hash": "bb98b072a58dc7e0",
            "position": "Top Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img14.jpeg",
            "hash": "efc2938c184ecee4",
            "position": "Top Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img15.jpeg",
            "hash": "b0e0d7c64c91d9ce",
            "position": "Middle Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img16.png",
            "hash": "8080800000000000",
            "position": "Bottom Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img17.png",
            "hash": "8000808000808000",
            "position": "Bottom Right"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img18.png",
            "hash": "8080d5d55555577f",
            "position": "Bottom Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img19.png",
            "hash": "8185d5d5d5555555",
            "position": "Bottom Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img20.png",
            "hash": "9595959595555555",
            "position": "Bottom Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img21.png",
            "hash": "c5c5d5d1d1554555",
            "position": "Bottom Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img23.png",
            "hash": "808080800000000a",
            "position": "Bottom Center"
        },
        {
            "page": 11,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page11_img24.png",
            "hash": "84721b8d7872bd0f",
            "position": "Bottom Right"
        },
        {
            "page": 12,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page12_img1.jpeg",
            "hash": "e4c85b32c636e58d",
            "position": "Bottom Right"
        },
        {
            "page": 12,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page12_img2.jpeg",
            "hash": "e5e30f5a78101e6b",
            "position": "Bottom Center"
        },
        {
            "page": 12,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page12_img3.jpeg",
            "hash": "e5c10f3d78383a63",
            "position": "Bottom Right"
        },
        {
            "page": 12,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page12_img4.jpeg",
            "hash": "e5e71f3e7030524a",
            "position": "Bottom Right"
        },
        {
            "page": 12,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page12_img5.jpeg",
            "hash": "84b4925b4267bf6a",
            "position": "Bottom Center"
        },
        {
            "page": 12,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page12_img6.png",
            "hash": "9069369fc9e47319",
            "position": "Bottom Right"
        },
        {
            "page": 13,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page13_img3.jpeg",
            "hash": "c3731c0f39c03f9c",
            "position": "Middle Right"
        },
        {
            "page": 13,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page13_img7.png",
            "hash": "bb98b070a58dc7e2",
            "position": "Middle Center"
        },
        {
            "page": 13,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page13_img8.png",
            "hash": "81550157055f17ff",
            "position": "Middle Center"
        },
        {
            "page": 13,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page13_img9.jpeg",
            "hash": "997932c46c934f99",
            "position": "Middle Center"
        },
        {
            "page": 13,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page13_img10.jpeg",
            "hash": "d906e6aeb8d96126",
            "position": "Middle Center"
        },
        {
            "page": 13,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page13_img12.jpeg",
            "hash": "d906e6aeb8d96126",
            "position": "Middle Right"
        },
        {
            "page": 13,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page13_img17.png",
            "hash": "84721f8d5872bc1f",
            "position": "Middle Right"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page14_img1.jpeg",
            "hash": "bcc2c29795893c6b",
            "position": "Middle Center"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page14_img2.jpeg",
            "hash": "e366dc99b1662253",
            "position": "Middle Right"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page14_img3.jpeg",
            "hash": "b338f1c297174e68",
            "position": "Middle Center"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page14_img4.jpeg",
            "hash": "d0b62f6dc0c0363f",
            "position": "Middle Right"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page14_img5.png",
            "hash": "858595d5d5555555",
            "position": "Middle Center"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page14_img7.png",
            "hash": "8080808080808080",
            "position": "Middle Center"
        },
        {
            "page": 14,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page14_img8.png",
            "hash": "8080808080808080",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img1.jpeg",
            "hash": "b338f1c297174e68",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img4.jpeg",
            "hash": "d0b62f6dc0c0363f",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img6.png",
            "hash": "8080000000808080",
            "position": "Middle Right"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img7.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Right"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img11.jpeg",
            "hash": "9ec961367825e6c6",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img12.jpeg",
            "hash": "9469272e6c91d96e",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img13.jpeg",
            "hash": "bcc2c29795893c6b",
            "position": "Middle Right"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img14.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img15.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img16.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img17.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img18.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img19.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img20.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Right"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img21.png",
            "hash": "80802a2a2a808080",
            "position": "Middle Right"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img22.png",
            "hash": "bb58b035a485cfe2",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img23.png",
            "hash": "bb58b035a485cfe2",
            "position": "Middle Center"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img24.jpeg",
            "hash": "e366dc99b1662253",
            "position": "Middle Right"
        },
        {
            "page": 15,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page15_img25.png",
            "hash": "bb58b03da4858fe2",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img1.jpeg",
            "hash": "ea95a5c491d19567",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img6.png",
            "hash": "bc2cd39be072d04d",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img7.jpeg",
            "hash": "d8a21ddee00d3f8c",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img8.png",
            "hash": "d21e70650df813f6",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img9.jpeg",
            "hash": "9db425e9061d5dc5",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img10.png",
            "hash": "8163b1787f8d568a",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img11.jpeg",
            "hash": "d4b52bd245534d4b",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img12.jpeg",
            "hash": "c0e01b0b3f8d3e6d",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img15.png",
            "hash": "bb98b072a58dc7e0",
            "position": "Top Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img17.png",
            "hash": "d21e70650df813f6",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img18.png",
            "hash": "8163b1787f8d568a",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img19.png",
            "hash": "f017e4690fe813f2",
            "position": "Top Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img20.jpeg",
            "hash": "9db425e9065d5d45",
            "position": "Top Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img21.png",
            "hash": "8163b1787f8d568a",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img22.png",
            "hash": "81550957255e35fb",
            "position": "Top Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img23.jpeg",
            "hash": "fbcb900491cbcb9c",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img24.jpeg",
            "hash": "f5ce901c492716f3",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img25.png",
            "hash": "8185d5d5d5555555",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img26.png",
            "hash": "8185d5d5d5555555",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img27.png",
            "hash": "d5d5d5d581055555",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img28.png",
            "hash": "8095d5d5d5555555",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img29.jpeg",
            "hash": "c4324bcd3e6c3c66",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img30.png",
            "hash": "9465b4e643e84e5e",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img31.png",
            "hash": "cea0314e4f3bc4f1",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img32.png",
            "hash": "c49d3b6831a52f66",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img33.jpeg",
            "hash": "9cc06676272e7335",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img34.jpeg",
            "hash": "ecc2b7f82acd68c0",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img35.jpeg",
            "hash": "ed5a94a03b3dc643",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img36.jpeg",
            "hash": "de6c21d3cb388665",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img39.jpeg",
            "hash": "8cb9314e4c30f6de",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img40.jpeg",
            "hash": "83bc699635699ab4",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img42.jpeg",
            "hash": "a04b8a3c63c79e3b",
            "position": "Middle Right"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img44.jpeg",
            "hash": "a1d7d02a60d53f9a",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img46.jpeg",
            "hash": "aa62dc715419e71e",
            "position": "Middle Center"
        },
        {
            "page": 16,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page16_img47.jpeg",
            "hash": "ec4a973668c99a56",
            "position": "Middle Right"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img1.jpeg",
            "hash": "862699997b272799",
            "position": "Top Right"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img2.jpeg",
            "hash": "e85993a6e67390cc",
            "position": "Top Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img3.jpeg",
            "hash": "ec4a973668c99a56",
            "position": "Middle Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img4.jpeg",
            "hash": "b131c7ec2839cbc3",
            "position": "Middle Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img5.jpeg",
            "hash": "e50cda3325ccda33",
            "position": "Top Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img6.jpeg",
            "hash": "da920949c0f3f9ec",
            "position": "Middle Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img7.jpeg",
            "hash": "e76c993366c83986",
            "position": "Top Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img8.jpeg",
            "hash": "91b566685bda9634",
            "position": "Middle Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img9.jpeg",
            "hash": "e389d9a322cd9966",
            "position": "Top Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img10.jpeg",
            "hash": "ee79ec693124c364",
            "position": "Middle Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img11.png",
            "hash": "9e966169996265cd",
            "position": "Top Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img12.png",
            "hash": "81550957055e35ff",
            "position": "Top Center"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img13.jpeg",
            "hash": "d4123132de4fb1e6",
            "position": "Bottom Right"
        },
        {
            "page": 17,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page17_img14.jpeg",
            "hash": "ccaadad5b152b24c",
            "position": "Middle Right"
        },
        {
            "page": 18,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page18_img2.jpeg",
            "hash": "b2a5cc5a8d5cc927",
            "position": "Top Center"
        },
        {
            "page": 18,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page18_img3.jpeg",
            "hash": "fab5904ac3a3c59c",
            "position": "Top Center"
        },
        {
            "page": 18,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page18_img4.png",
            "hash": "c6697962668c1fb1",
            "position": "Top Center"
        },
        {
            "page": 18,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page18_img7.jpeg",
            "hash": "9eadc0af4052ad73",
            "position": "Top Right"
        },
        {
            "page": 18,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page18_img8.png",
            "hash": "bbcac795b403e238",
            "position": "Top Right"
        },
        {
            "page": 19,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page19_img2.png",
            "hash": "e1621f8ce3b12c9b",
            "position": "Top Center"
        },
        {
            "page": 19,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page19_img4.png",
            "hash": "90e627936f0c697c",
            "position": "Top Left"
        },
        {
            "page": 19,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page19_img5.png",
            "hash": "9410437f4be9cf1c",
            "position": "Top Center"
        },
        {
            "page": 19,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page19_img7.png",
            "hash": "d0653e1967e2982f",
            "position": "Top Left"
        },
        {
            "page": 19,
            "image_file": "docNAC01FB7453534c73c701b9333c200aa22dd3d399dd42015128cf3961b7d5de73ae141029c385_page19_img8.png",
            "hash": "c1743ecb607c61c7",
            "position": "Top Right"
        }
    ],
    "firm_info": {
        "company": "N/A",
        "address": "N/A",
        "website": "ai-sensation.com",
        "name": "N/A",
        "phone": "N/A"
    }
}